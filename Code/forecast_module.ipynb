{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import LSTM , Bidirectional\n",
    "color_pal = sns.color_palette()\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed(id):\n",
    "    torch.manual_seed(id)\n",
    "    np.random.seed(id)\n",
    "    random.seed(id)\n",
    "    tf.random.set_seed(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>^GSPC_Open</th>\n",
       "      <th>^GSPC_Volume</th>\n",
       "      <th>^GSPC_SQZ_20_2.0_20_1.5</th>\n",
       "      <th>^GSPC_SQZ_ON</th>\n",
       "      <th>^GSPC_ABER_ATR_5_15</th>\n",
       "      <th>^GSPC_ADOSC_3_10</th>\n",
       "      <th>^GSPC_ADX_14</th>\n",
       "      <th>^GSPC_DMP_14</th>\n",
       "      <th>^GSPC_DMN_14</th>\n",
       "      <th>^GSPC_AROOND_14</th>\n",
       "      <th>...</th>\n",
       "      <th>DX-Y.NYB level 50</th>\n",
       "      <th>^GSPC_High</th>\n",
       "      <th>^GSPC_Low</th>\n",
       "      <th>^GSPC_Close</th>\n",
       "      <th>DX-Y.NYB_High</th>\n",
       "      <th>DX-Y.NYB_Low</th>\n",
       "      <th>DX-Y.NYB_Close</th>\n",
       "      <th>VIX_Close</th>\n",
       "      <th>IXIC_Close</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>438.670013</td>\n",
       "      <td>247200000</td>\n",
       "      <td>7.049998</td>\n",
       "      <td>False</td>\n",
       "      <td>2.902007</td>\n",
       "      <td>6.886895e+07</td>\n",
       "      <td>18.006061</td>\n",
       "      <td>28.075804</td>\n",
       "      <td>22.869827</td>\n",
       "      <td>7.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>438.929993</td>\n",
       "      <td>436.910004</td>\n",
       "      <td>438.779999</td>\n",
       "      <td>92.610001</td>\n",
       "      <td>91.080002</td>\n",
       "      <td>92.459999</td>\n",
       "      <td>12.42</td>\n",
       "      <td>696.340027</td>\n",
       "      <td>1993-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>438.779999</td>\n",
       "      <td>238570000</td>\n",
       "      <td>7.881663</td>\n",
       "      <td>False</td>\n",
       "      <td>2.957872</td>\n",
       "      <td>1.703827e+08</td>\n",
       "      <td>18.460351</td>\n",
       "      <td>34.209112</td>\n",
       "      <td>20.804444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>442.519989</td>\n",
       "      <td>438.779999</td>\n",
       "      <td>442.519989</td>\n",
       "      <td>93.730003</td>\n",
       "      <td>92.419998</td>\n",
       "      <td>93.559998</td>\n",
       "      <td>12.33</td>\n",
       "      <td>701.770020</td>\n",
       "      <td>1993-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>442.519989</td>\n",
       "      <td>271560000</td>\n",
       "      <td>7.234996</td>\n",
       "      <td>False</td>\n",
       "      <td>2.901346</td>\n",
       "      <td>2.566182e+08</td>\n",
       "      <td>18.970118</td>\n",
       "      <td>33.292546</td>\n",
       "      <td>19.722291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>442.869995</td>\n",
       "      <td>440.760010</td>\n",
       "      <td>442.549988</td>\n",
       "      <td>94.040001</td>\n",
       "      <td>93.199997</td>\n",
       "      <td>93.919998</td>\n",
       "      <td>12.25</td>\n",
       "      <td>705.119995</td>\n",
       "      <td>1993-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>442.559998</td>\n",
       "      <td>345410000</td>\n",
       "      <td>7.144994</td>\n",
       "      <td>False</td>\n",
       "      <td>3.027924</td>\n",
       "      <td>3.715875e+08</td>\n",
       "      <td>20.417289</td>\n",
       "      <td>40.078885</td>\n",
       "      <td>17.493099</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>447.350006</td>\n",
       "      <td>442.559998</td>\n",
       "      <td>447.200012</td>\n",
       "      <td>94.599998</td>\n",
       "      <td>93.599998</td>\n",
       "      <td>94.239998</td>\n",
       "      <td>12.12</td>\n",
       "      <td>708.669983</td>\n",
       "      <td>1993-02-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>447.200012</td>\n",
       "      <td>351140000</td>\n",
       "      <td>8.356664</td>\n",
       "      <td>False</td>\n",
       "      <td>3.003394</td>\n",
       "      <td>4.713655e+08</td>\n",
       "      <td>22.193214</td>\n",
       "      <td>43.509068</td>\n",
       "      <td>16.387681</td>\n",
       "      <td>28.571429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>449.859985</td>\n",
       "      <td>447.200012</td>\n",
       "      <td>449.559998</td>\n",
       "      <td>94.860001</td>\n",
       "      <td>94.040001</td>\n",
       "      <td>94.529999</td>\n",
       "      <td>12.29</td>\n",
       "      <td>708.849976</td>\n",
       "      <td>1993-02-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 246 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ^GSPC_Open  ^GSPC_Volume  ^GSPC_SQZ_20_2.0_20_1.5  ^GSPC_SQZ_ON  \\\n",
       "0  438.670013     247200000                 7.049998         False   \n",
       "1  438.779999     238570000                 7.881663         False   \n",
       "2  442.519989     271560000                 7.234996         False   \n",
       "3  442.559998     345410000                 7.144994         False   \n",
       "4  447.200012     351140000                 8.356664         False   \n",
       "\n",
       "   ^GSPC_ABER_ATR_5_15  ^GSPC_ADOSC_3_10  ^GSPC_ADX_14  ^GSPC_DMP_14  \\\n",
       "0             2.902007      6.886895e+07     18.006061     28.075804   \n",
       "1             2.957872      1.703827e+08     18.460351     34.209112   \n",
       "2             2.901346      2.566182e+08     18.970118     33.292546   \n",
       "3             3.027924      3.715875e+08     20.417289     40.078885   \n",
       "4             3.003394      4.713655e+08     22.193214     43.509068   \n",
       "\n",
       "   ^GSPC_DMN_14  ^GSPC_AROOND_14  ...  DX-Y.NYB level 50  ^GSPC_High  \\\n",
       "0     22.869827         7.142857  ...                0.0  438.929993   \n",
       "1     20.804444         0.000000  ...                0.0  442.519989   \n",
       "2     19.722291         0.000000  ...                0.0  442.869995   \n",
       "3     17.493099        35.714286  ...                0.0  447.350006   \n",
       "4     16.387681        28.571429  ...                0.0  449.859985   \n",
       "\n",
       "    ^GSPC_Low  ^GSPC_Close  DX-Y.NYB_High  DX-Y.NYB_Low  DX-Y.NYB_Close  \\\n",
       "0  436.910004   438.779999      92.610001     91.080002       92.459999   \n",
       "1  438.779999   442.519989      93.730003     92.419998       93.559998   \n",
       "2  440.760010   442.549988      94.040001     93.199997       93.919998   \n",
       "3  442.559998   447.200012      94.599998     93.599998       94.239998   \n",
       "4  447.200012   449.559998      94.860001     94.040001       94.529999   \n",
       "\n",
       "   VIX_Close  IXIC_Close        Date  \n",
       "0      12.42  696.340027  1993-01-29  \n",
       "1      12.33  701.770020  1993-02-01  \n",
       "2      12.25  705.119995  1993-02-02  \n",
       "3      12.12  708.669983  1993-02-03  \n",
       "4      12.29  708.849976  1993-02-04  \n",
       "\n",
       "[5 rows x 246 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('filtered_features.csv', index_col='Date')\n",
    "df['Date'] = df.index\n",
    "df.index = np.array(range(df.shape[0]))\n",
    "df = df.drop('Unnamed: 0', axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7534,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Date'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final k 120\n"
     ]
    }
   ],
   "source": [
    "def save_data(data, time_index_loc, name):\n",
    "    data = pd.DataFrame(data)            \n",
    "    data['Date'] = time_index_loc\n",
    "    data.to_csv(name)\n",
    "\n",
    "def series_data_split(data , time_index,  lag, horizon, t_train, t_val, t_test, name, step = 500):\n",
    "    # data - в виде серии\n",
    "     \n",
    "    data = np.array(data.to_list()).reshape(-1, 1)\n",
    "    indexes = np.array(range(len(data)))\n",
    "    \n",
    "    k = 0\n",
    "\n",
    "    while t_val < indexes[-1]:\n",
    "        x_train = []\n",
    "        y_train = []\n",
    "        x_test = []\n",
    "        y_test = []\n",
    "        x_val = []\n",
    "        y_val = []\n",
    "\n",
    "        if k == 0:\n",
    "           \n",
    "\n",
    "            scaled = np.array(data[: t_val]).reshape(1, -1)[0]\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            for i in range(lag + 1, t_train + 1):\n",
    "                x_train.append(np.log(scaled[i-lag:i] / scaled[i-lag-1:i-1])) \n",
    "                y_train.append(np.log(scaled[i]/scaled[i-1]))\n",
    "            \n",
    "            for i in range(t_train + 1, t_val):\n",
    "                x_val.append(np.log(scaled[i-lag:i] / scaled[i-lag-1:i-1]))\n",
    "                if i + horizon <= t_val:\n",
    "                    y_val.append(np.log(scaled[i : i + horizon]/scaled[i-1:i+horizon-1]))\n",
    "                else:\n",
    "                    y_val.append(np.log(scaled[i : t_val]/scaled[i-1:t_val-1]))\n",
    "            \n",
    "            \n",
    "            save_data(x_train, time_index.iloc[indexes[lag : t_train]].to_list(), f'x_train_v2_{name}_{k}.csv')\n",
    "            save_data(y_train, time_index.iloc[indexes[lag : t_train]].to_list(), f'y_train_v2_{name}_{k}.csv')\n",
    "            \n",
    "            \n",
    "            save_data(x_val, time_index.iloc[indexes[t_train: t_val - 1]].to_list(), f'x_val_v2_{name}_{k}.csv')\n",
    "            save_data(y_val, time_index.iloc[indexes[t_train: t_val - 1]].to_list(), f'y_val_v2_{name}_{k}.csv')\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "       \n",
    "        scaled_train_data = np.array(data[:t_val]).reshape(1, -1)[0]\n",
    "      \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        x_train = []\n",
    "        y_train = []\n",
    "        x_test = []\n",
    "        y_test = []\n",
    "        x_val = []\n",
    "        y_val = []\n",
    "\n",
    "        \n",
    "        for i in range(lag + 1, t_val):\n",
    "            x_train.append(np.log(scaled_train_data[i-lag:i]/scaled_train_data[i-lag-1:i-1]))\n",
    "            y_train.append(np.log(scaled_train_data[i]/scaled_train_data[i-1]))\n",
    "\n",
    "        save_data(x_train, time_index.iloc[indexes[lag : t_val - 1]].to_list(), f'x_train_v2_{name}_{k + 1}.csv')\n",
    "        save_data(y_train, time_index.iloc[indexes[lag : t_val - 1]].to_list(), f'y_train_v2_{name}_{k + 1}.csv')\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        scaled_test = np.array(data[:t_test])\n",
    "        scaled_test = scaled_test.reshape(1, -1)[0]\n",
    "\n",
    "        pointer = 0\n",
    "        for i in range(t_val, t_test):\n",
    "            \n",
    "            x_test.append(np.log(scaled_test[i-lag:i]/scaled_test[i-lag-1:i-1])) \n",
    "            if i + horizon <= t_test:\n",
    "                y_test.append(np.log(scaled_test[i : i + horizon]/scaled_test[i-1:i+horizon-1]))\n",
    "            else:\n",
    "                y_test.append(np.log(scaled_test[i : t_test]/scaled_test[i-1 : t_test-1]))\n",
    "            \n",
    "              \n",
    "       \n",
    "        save_data(x_test, time_index.iloc[indexes[t_val  - 1: t_test - 1]].to_list(), f'x_test_v2_{name}_{k + 1}.csv')\n",
    "        save_data(y_test, time_index.iloc[indexes[t_val  - 1: t_test - 1]].to_list(), f'y_test_v2_{name}_{k + 1}.csv')\n",
    "          \n",
    "        \n",
    "        \n",
    "     \n",
    "        k += 1\n",
    "        t_train = t_val\n",
    "        \n",
    "\n",
    "        t_val = t_test \n",
    "        t_test += step\n",
    "        if t_test > indexes[-1]:\n",
    "            t_test = indexes[-1] + 1\n",
    "\n",
    "           \n",
    "        \n",
    "    \n",
    "name = 'IXIC_Close' #'DX-Y.NYB_Close' 'VIX_Close       \n",
    "ser = df[name]\n",
    "\n",
    "time_index = df['Date']\n",
    "lag = 40\n",
    "horizon = 10\n",
    "t_train = 1500\n",
    "t_val = 1550\n",
    "t_test = 1600\n",
    "step = 50\n",
    "\n",
    "series_data_split(ser , time_index, lag, horizon, t_train, t_val, t_test, name, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_predictor:\n",
    "    def __init__(self, input_shape, seq_num, inner_output, output_shape, num_layers, drop, bilstm = False):\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        ret_seq = True if num_layers > 1 else False\n",
    "        if not bilstm:\n",
    "            self.model.add(LSTM(units = inner_output, return_sequences = ret_seq, input_shape = (input_shape, seq_num)))\n",
    "            self.model.add(Dropout(drop))\n",
    "            if num_layers > 1:\n",
    "                if num_layers > 2:\n",
    "                    for i in range(num_layers-2):\n",
    "                        self.model.add(LSTM(units = inner_output, return_sequences = True))\n",
    "                        self.model.add(Dropout(drop))\n",
    "\n",
    "                self.model.add(LSTM(units = inner_output))\n",
    "                self.model.add(Dropout(drop))\n",
    "            self.model.add(Dense(units = output_shape))\n",
    "        else:\n",
    "        \n",
    "            # First layer of BiLSTM\n",
    "            self.model.add(Bidirectional(LSTM(units = inner_output, return_sequences = ret_seq, input_shape = (input_shape, seq_num))))\n",
    "            self.model.add(Dropout(drop))\n",
    "            if num_layers > 1:\n",
    "                if num_layers > 2:\n",
    "                    for i in range(num_layers-2):\n",
    "                        self.model.add(Bidirectional(LSTM(units = inner_output, return_sequences = True)))\n",
    "                        self.model.add(Dropout(drop))\n",
    "\n",
    "            \n",
    "                # Second layer of BiLSTM\n",
    "                self.model.add(Bidirectional(LSTM(units = inner_output)))\n",
    "                self.model.add(Dropout(drop))\n",
    "            self.model.add(Dense(units = output_shape))\n",
    "            \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_wrapper_tf:\n",
    "    def __init__(self, nn_type):\n",
    "        self.type = nn_type\n",
    "        \n",
    "        \n",
    "    def set_params(self, **params):\n",
    "    \n",
    "        self.num_epochs = params['num_epochs']\n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.batch_size = params['batch_size']\n",
    "        self.hidden_size = params['hidden_size'] #, num_layers, seq_length, drop\n",
    "        self.num_layers = params['num_layers']\n",
    "        self.drop = params['drop']\n",
    "        if 'bilstm' in params.keys():\n",
    "\n",
    "            self.bilstm = params['bilstm']\n",
    "        else:\n",
    "            self.bilstm = False\n",
    "        \n",
    "\n",
    "    def fit(self, X, y, X_val = 0, y_val = 0):\n",
    "        \n",
    "        \n",
    "        if isinstance(X, np.ndarray):\n",
    "            self.X_train = np.reshape(X, (X.shape[1], X.shape[2], X.shape[0]))\n",
    "            self.y_train = np.array(y).reshape((len(y), 1))\n",
    "        else:\n",
    "            self.X_train = np.reshape(X.values, (X.shape[1], X.shape[2], X.shape[0]))\n",
    "            self.y_train = np.array(y.values).reshape((len(y), 1))\n",
    "        print(X.shape[2],'lll', X.shape[0])\n",
    "        # (self, input_shape, inner_output, output_shape, num_layers, drop)\n",
    "        self.nn = LSTM_predictor(X.shape[2], X.shape[0], self.hidden_size, 1, self.num_layers, self.drop, self.bilstm)\n",
    "        self.nn.model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "        self.nn.model.fit(self.X_train, self.y_train, epochs = self.num_epochs, batch_size = self.batch_size, shuffle = False)\n",
    "                \n",
    "                   \n",
    "    def predict(self, x):\n",
    "        \n",
    "        return self.nn.model.predict(np.reshape(x, (x.shape[1], x.shape[2], x.shape[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import optuna\n",
    "\n",
    "\n",
    "def optuna_hyper_opt(model, X_train, y_train, X_val, y_val, params, metric_weights, num_lags):\n",
    "    \n",
    "    \n",
    "    def objective(trial):\n",
    "        parameters = {}\n",
    "        for i in params:\n",
    "            \n",
    "            if type(params[i]) == list:\n",
    "                if type(params[i][0]) == int:\n",
    "                    parameters.update({i: trial.suggest_int(i, params[i][0], params[i][1])})\n",
    "                elif type(params[i][0]) == float:\n",
    "                    parameters.update({i: trial.suggest_float(i, params[i][0], params[i][1])})\n",
    "                elif type(params[i][0]) == str:\n",
    "                    parameters.update({i: trial.suggest_categorical(i, params[i])})\n",
    "            else:\n",
    "                parameters.update({i:params[i]})\n",
    "        \n",
    "        print(parameters)\n",
    "        \n",
    "        model.set_params(**parameters)\n",
    "\n",
    "        if isinstance(model, nn_wrapper_tf):\n",
    "            model.fit(X_train, y_train, X_val, y_val)\n",
    "        else:\n",
    "            print('k o')\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "        \n",
    "        \n",
    "        pred = horizon_forecasts_v2(model, X_val, y_val.shape[1], num_lags)\n",
    "        score = np.array(scores_fun(pred, y_val))\n",
    "\n",
    "        return np.dot(score, metric_weights)\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=3, timeout=300)\n",
    "    return study.best_params\n",
    "\n",
    "def horizon_forecasts_v2(model, x, horizon, num_lags):\n",
    "    \n",
    "    all_features = x\n",
    "    if isinstance(model, nn_wrapper_tf):\n",
    "        for i in range(horizon):\n",
    "            f = model.predict(all_features).reshape(-1, 1)\n",
    "            \n",
    "            if i == 0:\n",
    "                prediction = f\n",
    "            else: \n",
    "                prediction = np.concatenate((prediction, f), axis=1)\n",
    "            lags = all_features[0]\n",
    "            \n",
    "            not_lags = all_features[1:]\n",
    "            \n",
    "            lags = np.concatenate((lags[:, 1:], f), axis=1)\n",
    "           \n",
    "            \n",
    "            \n",
    "            all_features = np.concatenate([np.array([lags]), not_lags])\n",
    "    elif num_lags < x.shape[1]:\n",
    "        not_lags = all_features[:, num_lags:]\n",
    "        lags = all_features[:, :num_lags]\n",
    "        for i in range(horizon):\n",
    "            f = model.predict(all_features).reshape(-1, 1)\n",
    "            \n",
    "            if i == 0:\n",
    "                prediction = f\n",
    "            else: \n",
    "                prediction = np.concatenate((prediction, f), axis=1)\n",
    "            \n",
    "            lags = np.concatenate((lags[:, 1:], f), axis=1)\n",
    "            all_features = np.concatenate((lags, not_lags), axis=1)\n",
    "    else:\n",
    "        lags = all_features\n",
    "        for i in range(horizon):\n",
    "            f = model.predict(lags).reshape(-1, 1)\n",
    "            \n",
    "            if i == 0:\n",
    "                prediction = f\n",
    "            else: \n",
    "                prediction = np.concatenate((prediction, f), axis=1)\n",
    "\n",
    "            lags =  np.concatenate((lags[:, 1:], f), axis=1)\n",
    "\n",
    "    return np.array(prediction)\n",
    "\n",
    "def horizon_forecasts(model, x, horizon, num_lags):\n",
    "    prediction = []\n",
    "    all_features = x.reshape(1, -1)\n",
    "    if num_lags < len(x):\n",
    "        not_lags = all_features[0][num_lags:]\n",
    "        lags = all_features[0][:num_lags]\n",
    "        for i in range(horizon):\n",
    "            f = model.predict(all_features)[0]\n",
    "            prediction.append(f)\n",
    "            lags = np.append(lags[1:], f)\n",
    "            all_features = np.append(lags, not_lags).reshape(1, -1)\n",
    "            \n",
    "    else:\n",
    "        lags = all_features\n",
    "        for i in range(horizon):\n",
    "            f = model.predict(lags)[0]\n",
    "            prediction.append(f)\n",
    "            lags = np.append(lags[0][1:], f).reshape(1, -1)\n",
    "       \n",
    "       \n",
    "\n",
    "    return np.array(prediction)\n",
    "\n",
    "def scores_fun(pred, y):\n",
    "    horizon = y.shape[1]\n",
    "    n = y.shape[0]\n",
    "    metric_mse = 0\n",
    "    metric_trend_detect = 0\n",
    "    metric_weights = 0\n",
    "    metric_mape = 0\n",
    "    metric_true_pred = 0\n",
    "    \n",
    "    weights = np.array([i / horizon for i in range(1, horizon + 1)])\n",
    "\n",
    "    for i in range(n - horizon + 1):\n",
    "        diff = (pred[i] - y[i]) ** 2\n",
    "        if pred[i][0] * y[i][0] > 0:\n",
    "             metric_true_pred += 1\n",
    "        metric_mape += mean_absolute_percentage_error(y[i], pred[i])\n",
    "\n",
    "        metric_mse += np.sum(diff) / horizon\n",
    "        \n",
    "        if np.dot(np.sum(pred[i]), np.sum(y[i])) > 0:\n",
    "            metric_trend_detect += 1\n",
    "        \n",
    "        metric_weights += np.sum(np.dot(weights, diff)) / horizon\n",
    "\n",
    "    for ind, j in enumerate(range(n - horizon + 1, n)):\n",
    "        s1 = 0\n",
    "        s1_w = 0\n",
    "        s_mape = 0\n",
    "        s_tr_det = 0\n",
    "        s_tr_det_pred = 0\n",
    "        for i in range(horizon - ind - 1):\n",
    "            diff = (pred[j][i] - y[j][i]) ** 2 \n",
    "            s1 += diff\n",
    "            if i == 0:\n",
    "                if pred[j][i] * y[j][i] > 0:\n",
    "                    metric_true_pred += 1\n",
    "            if y[j][i] != 0:\n",
    "                s_mape += np.abs(pred[j][i] - y[j][i]) / y[j][i]\n",
    "            s1_w = diff * weights[i]\n",
    "            s_tr_det += y[j][i]\n",
    "            s_tr_det_pred += pred[j][i]\n",
    "            if i == horizon - ind - 1 - 1:\n",
    "                if np.dot(s_tr_det_pred, s_tr_det ) > 0:\n",
    "                    metric_trend_detect += 1 - ind / horizon\n",
    "        \n",
    "        metric_mse += s1 / (horizon - ind - 1)\n",
    "        metric_weights += s1_w / (horizon - ind -1)\n",
    "        metric_mape += s_mape / (horizon - ind - 1)\n",
    "\n",
    "\n",
    "    return [metric_mse/n, metric_weights/n, metric_trend_detect/n, 100 * metric_mape / n, metric_true_pred / n]\n",
    "\n",
    "\n",
    "def horizon_prediction(model, X, y, k, num_lags, addit_data=0):\n",
    "    time_index = X['Date']\n",
    "    if isinstance(model, nn_wrapper_tf):\n",
    "        X = X.drop('Date', axis = 1).values\n",
    "        X = np.concatenate([np.array([X]), np.array(addit_data)])\n",
    "        y = y.drop('Date', axis = 1).values\n",
    "    else:\n",
    "        X = X.drop('Date', axis = 1).values\n",
    "        y = y.drop('Date', axis = 1).values\n",
    "\n",
    "\n",
    "    hor = y.shape[1]\n",
    "    pred = []\n",
    "  \n",
    "    \n",
    "    pred = horizon_forecasts_v2(model, X, hor, num_lags)\n",
    "    # pred в df превратить\n",
    "    score = scores_fun(pred, y)\n",
    "    \n",
    "    score.append(time_index[0])\n",
    "    score.append(time_index[time_index.shape[0] - 1])\n",
    "  \n",
    "    pred = pd.DataFrame(pred, index=time_index)\n",
    "    return pred, score\n",
    "\n",
    "\n",
    "def model_forecasts(model, params_set, k, HPO=False, metric_weights = np.array([0, 0, 0, 0, 0])):\n",
    "    forecasts = []\n",
    "    metrics = []\n",
    "    df = pd.read_csv('filtered_features.csv', index_col='Date')\n",
    "    df['Date'] = df.index\n",
    "    df.index = np.array(range(df.shape[0]))\n",
    "    df = df.drop(['Unnamed: 0'], axis = 1)\n",
    "    tick = ['IXIC_Close', 'DX-Y.NYB_Close', 'VIX_Close']\n",
    "    for i in range(k):\n",
    "\n",
    "        print(i)\n",
    "        if i == 0:\n",
    "            data_x_train_val = pd.read_csv(f'./x_train_v2_/x_train_v2_{i}.csv').drop(['Unnamed: 0', 'Date'], axis = 1)\n",
    "            num_lags = data_x_train_val.shape[1] - 1\n",
    "            if isinstance(model, nn_wrapper_tf):\n",
    "                \n",
    "                addit_data = []\n",
    "                for t in tick:\n",
    "                    add_data = pd.read_csv(f'./x_train_v2_{t}_/x_train_v2_{t}_{i}.csv', index_col='Date')\n",
    "                    add_data['Date'] = add_data.index\n",
    "                    add_data.index = np.array(range(add_data.shape[0]))\n",
    "                    add_data = add_data.drop(['Unnamed: 0', 'Date'], axis = 1)\n",
    "                    \n",
    "                    addit_data.append(add_data.values)\n",
    "                \n",
    "                data_x_train_val = np.concatenate([np.array([data_x_train_val.values]), np.array(addit_data)])\n",
    "            else:\n",
    "                data_x_train_val = pd.merge(data_x_train_val, df, 'inner', 'Date').drop('Date', axis=1)\n",
    "\n",
    "            data_y_train_val = pd.read_csv(f'./y_train_v2_/y_train_v2_{i}.csv').drop(['Unnamed: 0', 'Date'], axis = 1)\n",
    "            data_x_val = pd.read_csv(f'./x_val_v2_{i}.csv').drop(['Unnamed: 0'], axis = 1)\n",
    "            if isinstance(model, nn_wrapper_tf):\n",
    "                \n",
    "                addit_data = []\n",
    "                for t in tick:\n",
    "                    add_data = pd.read_csv(f'./x_val_v2_{t}_{i}.csv', index_col='Date')\n",
    "                    add_data['Date'] = add_data.index\n",
    "                    add_data.index = np.array(range(add_data.shape[0]))\n",
    "                    add_data = add_data.drop(['Unnamed: 0', 'Date'], axis = 1)\n",
    "                    addit_data.append(add_data.values)\n",
    "                data_x_val = np.array([data_x_val.values, addit_data])\n",
    "                \n",
    "            else:\n",
    "                data_x_val = pd.merge(data_x_val, df, 'inner', 'Date').drop('Date', axis=1)\n",
    "\n",
    "            data_y_val = pd.read_csv(f'y_val_v2_{i}.csv').drop(['Unnamed: 0', 'Date'], axis = 1)\n",
    "        else:\n",
    "            data_x_train_val = data_x_train_test\n",
    "            data_y_train_val = data_y_train_test\n",
    "            data_x_val = data_x_test.drop( 'Date', axis = 1)\n",
    "            data_y_val = data_y_test.drop('Date', axis = 1)\n",
    "        \n",
    "        if HPO:\n",
    "            params = optuna_hyper_opt(model, data_x_train_val.values, data_y_train_val.values, data_x_val.values, data_y_val.values, params_set, metric_weights, num_lags)\n",
    "            for j in params_set:\n",
    "                if j not in params.keys():\n",
    "                    params.update({j : params_set[j]})    \n",
    "        else:\n",
    "            params = params_set \n",
    "\n",
    "        print(params)\n",
    "        model.set_params(**params)\n",
    "        \n",
    "\n",
    "        data_x_train_test = pd.read_csv(f'./x_train_v2_/x_train_v2_{i + 1}.csv').drop(['Unnamed: 0'], axis = 1)\n",
    "        if isinstance(model, nn_wrapper_tf):\n",
    "            addit_data = []\n",
    "            for t in tick:\n",
    "                add_data = pd.read_csv(f'./x_train_v2_{t}_/x_train_v2_{t}_{i + 1}.csv', index_col='Date')\n",
    "                add_data['Date'] = add_data.index\n",
    "                add_data.index = np.array(range(add_data.shape[0]))\n",
    "                add_data = add_data.drop(['Unnamed: 0', 'Date'], axis = 1)\n",
    "                addit_data.append(add_data.values)\n",
    "            \n",
    "            data_x_train_test = np.concatenate([np.array([data_x_train_test.drop(['Date'], axis = 1).values]), np.array(addit_data)])\n",
    "        else:\n",
    "            data_x_train_test = pd.merge(data_x_train_test, df, 'inner', 'Date').drop('Date', axis=1)\n",
    "            \n",
    "        data_y_train_test = pd.read_csv(f'./y_train_v2_/y_train_v2_{i + 1}.csv').drop(['Unnamed: 0', 'Date'], axis = 1)\n",
    "        data_x_test = pd.read_csv(f'./x_test_v2_/x_test_v2_{i + 1}.csv').drop('Unnamed: 0', axis = 1)\n",
    "        if not isinstance(model, nn_wrapper_tf):\n",
    "            data_x_test = pd.merge(data_x_test, df, 'inner', 'Date')\n",
    "        else:\n",
    "            addit_data = []\n",
    "            for t in tick:\n",
    "                add_data = pd.read_csv(f'./x_test_v2_{t}_/x_test_v2_{t}_{i + 1}.csv', index_col='Date')\n",
    "                add_data['Date'] = add_data.index\n",
    "                add_data.index = np.array(range(add_data.shape[0]))\n",
    "                add_data = add_data.drop(['Unnamed: 0', 'Date'], axis = 1)\n",
    "                addit_data.append(add_data.values)\n",
    "\n",
    "        data_y_test = pd.read_csv(f'./y_test_v2_/y_test_v2_{i + 1}.csv').drop('Unnamed: 0', axis = 1)\n",
    "        \n",
    "        model.fit(data_x_train_test, data_y_train_test)\n",
    "        res = horizon_prediction(model, data_x_test, data_y_test, i + 1, num_lags, addit_data)\n",
    "\n",
    "        \n",
    "        forecasts.append(res[0])\n",
    "        metrics.append(res[1])\n",
    "        print(i)\n",
    "    return pd.concat(forecasts), pd.DataFrame(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM 1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vitalij\\AppData\\Local\\Temp\\ipykernel_28048\\2125418757.py:241: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data_x_val = np.array([data_x_val.values, addit_data])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - 5s 29ms/step - loss: 1.0708e-04\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 8.1627e-05\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 8.1962e-05\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 8.2107e-05\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 8.1997e-05\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 8.1047e-05\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 8.2136e-05\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 8.2666e-05\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 8.0770e-05\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 8.0626e-05\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "0\n",
      "1\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "25/25 [==============================] - 5s 35ms/step - loss: 9.9558e-05\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 8.2148e-05\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 1s 27ms/step - loss: 8.6745e-05\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 1s 28ms/step - loss: 8.4540e-05\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 1s 28ms/step - loss: 8.3438e-05\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 1s 28ms/step - loss: 8.5150e-05\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 1s 26ms/step - loss: 8.3238e-05\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 1s 25ms/step - loss: 8.5750e-05\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 1s 26ms/step - loss: 8.1704e-05\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 1s 26ms/step - loss: 8.3181e-05\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "1\n",
      "2\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "26/26 [==============================] - 5s 28ms/step - loss: 1.0321e-04\n",
      "Epoch 2/10\n",
      "26/26 [==============================] - 1s 28ms/step - loss: 8.3865e-05\n",
      "Epoch 3/10\n",
      "26/26 [==============================] - 1s 28ms/step - loss: 8.5890e-05\n",
      "Epoch 4/10\n",
      "26/26 [==============================] - 1s 28ms/step - loss: 8.5319e-05\n",
      "Epoch 5/10\n",
      "26/26 [==============================] - 1s 28ms/step - loss: 8.6606e-05\n",
      "Epoch 6/10\n",
      "26/26 [==============================] - 1s 29ms/step - loss: 8.3131e-05\n",
      "Epoch 7/10\n",
      "26/26 [==============================] - 1s 28ms/step - loss: 8.5334e-05\n",
      "Epoch 8/10\n",
      "26/26 [==============================] - 1s 28ms/step - loss: 8.6130e-05\n",
      "Epoch 9/10\n",
      "26/26 [==============================] - 1s 28ms/step - loss: 8.6147e-05\n",
      "Epoch 10/10\n",
      "26/26 [==============================] - 1s 28ms/step - loss: 8.4123e-05\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2\n",
      "3\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "26/26 [==============================] - 5s 33ms/step - loss: 1.0197e-04\n",
      "Epoch 2/10\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 8.7924e-05\n",
      "Epoch 3/10\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 8.8940e-05\n",
      "Epoch 4/10\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 8.7516e-05\n",
      "Epoch 5/10\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 8.8080e-05\n",
      "Epoch 6/10\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 8.9209e-05\n",
      "Epoch 7/10\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 8.6802e-05\n",
      "Epoch 8/10\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 8.7303e-05\n",
      "Epoch 9/10\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 8.7070e-05\n",
      "Epoch 10/10\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 8.7432e-05\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "3\n",
      "4\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "27/27 [==============================] - 5s 32ms/step - loss: 1.1142e-04\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 8.4587e-05\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 8.6703e-05\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 1s 34ms/step - loss: 8.7044e-05\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 1s 35ms/step - loss: 8.5698e-05\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 1s 34ms/step - loss: 8.6733e-05\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 1s 36ms/step - loss: 8.7210e-05\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 8.8577e-05\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 8.7341e-05\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 1s 34ms/step - loss: 8.5929e-05\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "4\n",
      "5\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "28/28 [==============================] - 5s 32ms/step - loss: 1.0182e-04\n",
      "Epoch 2/10\n",
      "28/28 [==============================] - 1s 31ms/step - loss: 9.1124e-05\n",
      "Epoch 3/10\n",
      "28/28 [==============================] - 1s 32ms/step - loss: 9.3031e-05\n",
      "Epoch 4/10\n",
      "28/28 [==============================] - 1s 31ms/step - loss: 9.2315e-05\n",
      "Epoch 5/10\n",
      "28/28 [==============================] - 1s 31ms/step - loss: 9.5334e-05\n",
      "Epoch 6/10\n",
      "28/28 [==============================] - 1s 31ms/step - loss: 9.2626e-05\n",
      "Epoch 7/10\n",
      "28/28 [==============================] - 1s 31ms/step - loss: 9.2128e-05\n",
      "Epoch 8/10\n",
      "28/28 [==============================] - 1s 31ms/step - loss: 9.1219e-05\n",
      "Epoch 9/10\n",
      "28/28 [==============================] - 1s 31ms/step - loss: 9.0852e-05\n",
      "Epoch 10/10\n",
      "28/28 [==============================] - 1s 31ms/step - loss: 9.0698e-05\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "5\n",
      "6\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 5s 31ms/step - loss: 1.1206e-04\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 1s 30ms/step - loss: 9.9396e-05\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 1s 30ms/step - loss: 9.8765e-05\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 1s 29ms/step - loss: 1.0044e-04\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 1s 29ms/step - loss: 9.7819e-05\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 1s 30ms/step - loss: 9.7372e-05\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 1s 31ms/step - loss: 9.7323e-05\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 1s 30ms/step - loss: 9.8978e-05\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 1s 30ms/step - loss: 9.6740e-05\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 1s 30ms/step - loss: 9.7037e-05\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "6\n",
      "7\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "30/30 [==============================] - 5s 31ms/step - loss: 1.1277e-04\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 1.0487e-04\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 1.0165e-04\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 1.0022e-04\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 9.9423e-05\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 9.8691e-05\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 1.0329e-04\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 1.0064e-04\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 9.9624e-05\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 9.9511e-05\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "7\n",
      "8\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "30/30 [==============================] - 5s 31ms/step - loss: 1.1647e-04\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 1.0131e-04\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 9.7890e-05\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 1.0056e-04\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 9.8472e-05\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 9.6862e-05\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 9.8092e-05\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 9.9293e-05\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 1.0010e-04\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 9.8924e-05\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "8\n",
      "9\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "31/31 [==============================] - 5s 32ms/step - loss: 1.1686e-04\n",
      "Epoch 2/10\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 1.0233e-04\n",
      "Epoch 3/10\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 1.0325e-04\n",
      "Epoch 4/10\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 1.0418e-04\n",
      "Epoch 5/10\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 1.0195e-04\n",
      "Epoch 6/10\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 1.0416e-04\n",
      "Epoch 7/10\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 1.0169e-04\n",
      "Epoch 8/10\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 1.0252e-04\n",
      "Epoch 9/10\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 1.0213e-04\n",
      "Epoch 10/10\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 1.0247e-04\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "9\n",
      "10\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 5s 30ms/step - loss: 1.1622e-04\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 1.0495e-04\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 1.1003e-04\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 1.0702e-04\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 1.0466e-04\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 1.0768e-04\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 1.0771e-04\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 1.0530e-04\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 1.0562e-04\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 1.0678e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "10\n",
      "11\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "33/33 [==============================] - 5s 29ms/step - loss: 1.2358e-04\n",
      "Epoch 2/10\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 1.1408e-04\n",
      "Epoch 3/10\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 1.1446e-04\n",
      "Epoch 4/10\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 1.1151e-04\n",
      "Epoch 5/10\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 1.1326e-04\n",
      "Epoch 6/10\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 1.0976e-04\n",
      "Epoch 7/10\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 1.1128e-04\n",
      "Epoch 8/10\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 1.0863e-04\n",
      "Epoch 9/10\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 1.0934e-04\n",
      "Epoch 10/10\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 1.0848e-04\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "11\n",
      "12\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "33/33 [==============================] - 5s 31ms/step - loss: 1.2979e-04\n",
      "Epoch 2/10\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 1.0917e-04\n",
      "Epoch 3/10\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 1.0840e-04\n",
      "Epoch 4/10\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 1.0868e-04\n",
      "Epoch 5/10\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 1.1138e-04\n",
      "Epoch 6/10\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 1.0910e-04\n",
      "Epoch 7/10\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 1.0855e-04\n",
      "Epoch 8/10\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 1.0862e-04\n",
      "Epoch 9/10\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 1.0715e-04\n",
      "Epoch 10/10\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 1.0955e-04\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "12\n",
      "13\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "34/34 [==============================] - 5s 33ms/step - loss: 1.2704e-04\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - 1s 33ms/step - loss: 1.1380e-04\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - 1s 36ms/step - loss: 1.1165e-04\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - 1s 33ms/step - loss: 1.1093e-04\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - 1s 33ms/step - loss: 1.1088e-04\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - 1s 33ms/step - loss: 1.1111e-04\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - 1s 33ms/step - loss: 1.0995e-04\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - 1s 33ms/step - loss: 1.1211e-04\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - 1s 33ms/step - loss: 1.1012e-04\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - 1s 33ms/step - loss: 1.1114e-04\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "13\n",
      "14\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "35/35 [==============================] - 5s 32ms/step - loss: 1.2120e-04\n",
      "Epoch 2/10\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 1.1442e-04\n",
      "Epoch 3/10\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 1.1141e-04\n",
      "Epoch 4/10\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 1.1460e-04\n",
      "Epoch 5/10\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 1.1316e-04\n",
      "Epoch 6/10\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 1.1347e-04\n",
      "Epoch 7/10\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 1.1235e-04\n",
      "Epoch 8/10\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 1.1349e-04\n",
      "Epoch 9/10\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 1.1081e-04\n",
      "Epoch 10/10\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 1.1040e-04\n",
      "2/2 [==============================] - 1s 5ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "14\n",
      "15\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "36/36 [==============================] - 5s 32ms/step - loss: 1.2269e-04\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 1.1479e-04\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 1.1283e-04\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 1.1463e-04\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 1.1339e-04\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 1.1284e-04\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 1.1418e-04\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 1.1164e-04\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 1s 32ms/step - loss: 1.1125e-04\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 1.1257e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "15\n",
      "16\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "37/37 [==============================] - 5s 34ms/step - loss: 1.2362e-04\n",
      "Epoch 2/10\n",
      "37/37 [==============================] - 1s 34ms/step - loss: 1.1791e-04\n",
      "Epoch 3/10\n",
      "37/37 [==============================] - 1s 33ms/step - loss: 1.1448e-04\n",
      "Epoch 4/10\n",
      "37/37 [==============================] - 1s 34ms/step - loss: 1.1725e-04\n",
      "Epoch 5/10\n",
      "37/37 [==============================] - 1s 33ms/step - loss: 1.1598e-04\n",
      "Epoch 6/10\n",
      "37/37 [==============================] - 1s 33ms/step - loss: 1.1565e-04\n",
      "Epoch 7/10\n",
      "37/37 [==============================] - 1s 35ms/step - loss: 1.1564e-04\n",
      "Epoch 8/10\n",
      "37/37 [==============================] - 1s 34ms/step - loss: 1.1407e-04\n",
      "Epoch 9/10\n",
      "37/37 [==============================] - 1s 33ms/step - loss: 1.1338e-04\n",
      "Epoch 10/10\n",
      "37/37 [==============================] - 1s 34ms/step - loss: 1.1356e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "16\n",
      "17\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "37/37 [==============================] - 5s 32ms/step - loss: 1.2873e-04\n",
      "Epoch 2/10\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1.2104e-04\n",
      "Epoch 3/10\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1.2133e-04\n",
      "Epoch 4/10\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1.2068e-04\n",
      "Epoch 5/10\n",
      "37/37 [==============================] - 1s 33ms/step - loss: 1.1919e-04\n",
      "Epoch 6/10\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1.2108e-04\n",
      "Epoch 7/10\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1.1988e-04\n",
      "Epoch 8/10\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1.1996e-04\n",
      "Epoch 9/10\n",
      "37/37 [==============================] - 1s 33ms/step - loss: 1.1896e-04\n",
      "Epoch 10/10\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 1.1931e-04\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "17\n",
      "18\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "38/38 [==============================] - 5s 33ms/step - loss: 1.4454e-04\n",
      "Epoch 2/10\n",
      "38/38 [==============================] - 1s 32ms/step - loss: 1.2716e-04\n",
      "Epoch 3/10\n",
      "38/38 [==============================] - 1s 32ms/step - loss: 1.2748e-04\n",
      "Epoch 4/10\n",
      "38/38 [==============================] - 1s 32ms/step - loss: 1.2974e-04\n",
      "Epoch 5/10\n",
      "38/38 [==============================] - 1s 32ms/step - loss: 1.2873e-04\n",
      "Epoch 6/10\n",
      "38/38 [==============================] - 1s 33ms/step - loss: 1.2894e-04\n",
      "Epoch 7/10\n",
      "38/38 [==============================] - 1s 32ms/step - loss: 1.2631e-04\n",
      "Epoch 8/10\n",
      "38/38 [==============================] - 1s 33ms/step - loss: 1.2665e-04\n",
      "Epoch 9/10\n",
      "38/38 [==============================] - 1s 32ms/step - loss: 1.2540e-04\n",
      "Epoch 10/10\n",
      "38/38 [==============================] - 1s 33ms/step - loss: 1.2763e-04\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "18\n",
      "19\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "39/39 [==============================] - 5s 34ms/step - loss: 1.4391e-04\n",
      "Epoch 2/10\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 1.2603e-04\n",
      "Epoch 3/10\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 1.2886e-04\n",
      "Epoch 4/10\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 1.2973e-04\n",
      "Epoch 5/10\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 1.2892e-04\n",
      "Epoch 6/10\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 1.2915e-04\n",
      "Epoch 7/10\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 1.2866e-04\n",
      "Epoch 8/10\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 1.2738e-04\n",
      "Epoch 9/10\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 1.2806e-04\n",
      "Epoch 10/10\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 1.2726e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "19\n",
      "20\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "40/40 [==============================] - 5s 32ms/step - loss: 1.4043e-04\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - 1s 34ms/step - loss: 1.3188e-04\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 1s 31ms/step - loss: 1.3075e-04\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 1s 33ms/step - loss: 1.3212e-04\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 1s 34ms/step - loss: 1.3082e-04\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 1s 33ms/step - loss: 1.2934e-04\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 1s 33ms/step - loss: 1.3043e-04\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 1s 34ms/step - loss: 1.2969e-04\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 1s 35ms/step - loss: 1.2886e-04\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 1.2885e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "20\n",
      "21\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "40/40 [==============================] - 6s 41ms/step - loss: 1.4169e-04\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - 2s 38ms/step - loss: 1.3074e-04\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 1.3225e-04\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 1s 35ms/step - loss: 1.3274e-04\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 1.3030e-04\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 1.3124e-04\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 1.2975e-04\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 1.3116e-04\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 1.3100e-04\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 1.2893e-04\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "21\n",
      "22\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "41/41 [==============================] - 6s 37ms/step - loss: 1.4163e-04\n",
      "Epoch 2/10\n",
      "41/41 [==============================] - 2s 44ms/step - loss: 1.3625e-04\n",
      "Epoch 3/10\n",
      "41/41 [==============================] - 2s 38ms/step - loss: 1.3243e-04\n",
      "Epoch 4/10\n",
      "41/41 [==============================] - 1s 36ms/step - loss: 1.3232e-04\n",
      "Epoch 5/10\n",
      "41/41 [==============================] - 1s 36ms/step - loss: 1.3284e-04\n",
      "Epoch 6/10\n",
      "41/41 [==============================] - 1s 35ms/step - loss: 1.3194e-04\n",
      "Epoch 7/10\n",
      "41/41 [==============================] - 2s 36ms/step - loss: 1.3134e-04\n",
      "Epoch 8/10\n",
      "41/41 [==============================] - 1s 36ms/step - loss: 1.3131e-04\n",
      "Epoch 9/10\n",
      "41/41 [==============================] - 1s 35ms/step - loss: 1.3088e-04\n",
      "Epoch 10/10\n",
      "41/41 [==============================] - 1s 35ms/step - loss: 1.2956e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "22\n",
      "23\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "42/42 [==============================] - 7s 39ms/step - loss: 1.3739e-04\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - 2s 43ms/step - loss: 1.3261e-04\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - 2s 42ms/step - loss: 1.3294e-04\n",
      "Epoch 4/10\n",
      "42/42 [==============================] - 2s 41ms/step - loss: 1.3220e-04\n",
      "Epoch 5/10\n",
      "42/42 [==============================] - 2s 40ms/step - loss: 1.2879e-04\n",
      "Epoch 6/10\n",
      "42/42 [==============================] - 2s 39ms/step - loss: 1.3104e-04\n",
      "Epoch 7/10\n",
      "42/42 [==============================] - 2s 40ms/step - loss: 1.2913e-04\n",
      "Epoch 8/10\n",
      "42/42 [==============================] - 2s 40ms/step - loss: 1.2977e-04\n",
      "Epoch 9/10\n",
      "42/42 [==============================] - 2s 39ms/step - loss: 1.2674e-04\n",
      "Epoch 10/10\n",
      "42/42 [==============================] - 2s 40ms/step - loss: 1.2787e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "23\n",
      "24\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "43/43 [==============================] - 6s 40ms/step - loss: 1.3324e-04\n",
      "Epoch 2/10\n",
      "43/43 [==============================] - 2s 39ms/step - loss: 1.3142e-04\n",
      "Epoch 3/10\n",
      "43/43 [==============================] - 2s 37ms/step - loss: 1.3064e-04\n",
      "Epoch 4/10\n",
      "43/43 [==============================] - 2s 38ms/step - loss: 1.3155e-04\n",
      "Epoch 5/10\n",
      "43/43 [==============================] - 2s 37ms/step - loss: 1.2865e-04\n",
      "Epoch 6/10\n",
      "43/43 [==============================] - 2s 38ms/step - loss: 1.2794e-04\n",
      "Epoch 7/10\n",
      "43/43 [==============================] - 2s 37ms/step - loss: 1.2842e-04\n",
      "Epoch 8/10\n",
      "43/43 [==============================] - 2s 37ms/step - loss: 1.2823e-04\n",
      "Epoch 9/10\n",
      "43/43 [==============================] - 2s 36ms/step - loss: 1.2764e-04\n",
      "Epoch 10/10\n",
      "43/43 [==============================] - 2s 36ms/step - loss: 1.2870e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "24\n",
      "25\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "44/44 [==============================] - 6s 37ms/step - loss: 1.3775e-04\n",
      "Epoch 2/10\n",
      "44/44 [==============================] - 2s 36ms/step - loss: 1.2760e-04\n",
      "Epoch 3/10\n",
      "44/44 [==============================] - 2s 37ms/step - loss: 1.2848e-04\n",
      "Epoch 4/10\n",
      "44/44 [==============================] - 2s 38ms/step - loss: 1.2809e-04\n",
      "Epoch 5/10\n",
      "44/44 [==============================] - 2s 35ms/step - loss: 1.2687e-04\n",
      "Epoch 6/10\n",
      "44/44 [==============================] - 2s 39ms/step - loss: 1.2723e-04\n",
      "Epoch 7/10\n",
      "44/44 [==============================] - 2s 50ms/step - loss: 1.2640e-04\n",
      "Epoch 8/10\n",
      "44/44 [==============================] - 3s 59ms/step - loss: 1.2550e-04\n",
      "Epoch 9/10\n",
      "44/44 [==============================] - 2s 42ms/step - loss: 1.2444e-04\n",
      "Epoch 10/10\n",
      "44/44 [==============================] - 2s 38ms/step - loss: 1.2534e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "25\n",
      "26\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "44/44 [==============================] - 5s 30ms/step - loss: 1.3257e-04\n",
      "Epoch 2/10\n",
      "44/44 [==============================] - 1s 32ms/step - loss: 1.2736e-04\n",
      "Epoch 3/10\n",
      "44/44 [==============================] - 1s 29ms/step - loss: 1.2569e-04\n",
      "Epoch 4/10\n",
      "44/44 [==============================] - 1s 30ms/step - loss: 1.2649e-04\n",
      "Epoch 5/10\n",
      "44/44 [==============================] - 1s 30ms/step - loss: 1.2645e-04\n",
      "Epoch 6/10\n",
      "44/44 [==============================] - 1s 30ms/step - loss: 1.2693e-04\n",
      "Epoch 7/10\n",
      "44/44 [==============================] - 1s 30ms/step - loss: 1.2438e-04\n",
      "Epoch 8/10\n",
      "44/44 [==============================] - 1s 30ms/step - loss: 1.2501e-04\n",
      "Epoch 9/10\n",
      "44/44 [==============================] - 1s 29ms/step - loss: 1.2393e-04\n",
      "Epoch 10/10\n",
      "44/44 [==============================] - 1s 30ms/step - loss: 1.2433e-04\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 35ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "26\n",
      "27\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 5s 32ms/step - loss: 1.4116e-04\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 1s 32ms/step - loss: 1.2426e-04\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 1s 31ms/step - loss: 1.2460e-04\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 1s 31ms/step - loss: 1.2454e-04\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 1s 32ms/step - loss: 1.2298e-04\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 1s 31ms/step - loss: 1.2457e-04\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 1s 31ms/step - loss: 1.2253e-04\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 1s 31ms/step - loss: 1.2356e-04\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 1s 31ms/step - loss: 1.2347e-04\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 1s 32ms/step - loss: 1.2338e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "27\n",
      "28\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "46/46 [==============================] - 5s 33ms/step - loss: 1.2987e-04\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 1.2582e-04\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 1.2172e-04\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 1.2615e-04\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 1.2271e-04\n",
      "Epoch 6/10\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 1.2289e-04\n",
      "Epoch 7/10\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 1.2271e-04\n",
      "Epoch 8/10\n",
      "46/46 [==============================] - 1s 31ms/step - loss: 1.2186e-04\n",
      "Epoch 9/10\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 1.2199e-04\n",
      "Epoch 10/10\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 1.2222e-04\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "28\n",
      "29\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "47/47 [==============================] - 5s 32ms/step - loss: 1.3821e-04\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 1s 30ms/step - loss: 1.2260e-04\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 1s 30ms/step - loss: 1.2203e-04\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 1s 30ms/step - loss: 1.2067e-04\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 1s 30ms/step - loss: 1.2063e-04\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 2s 32ms/step - loss: 1.2152e-04\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 1s 30ms/step - loss: 1.2146e-04\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 1s 30ms/step - loss: 1.1976e-04\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 1s 30ms/step - loss: 1.2106e-04\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 1s 30ms/step - loss: 1.1893e-04\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "29\n",
      "30\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 5s 31ms/step - loss: 1.3452e-04\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 1.2064e-04\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 1.2146e-04\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 1.2164e-04\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 1.1953e-04\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 1.2052e-04\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 1.1925e-04\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 1.2068e-04\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 1.1855e-04\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 2s 31ms/step - loss: 1.2062e-04\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "30\n",
      "31\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 5s 36ms/step - loss: 1.2977e-04\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 2s 37ms/step - loss: 1.1927e-04\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 1.2144e-04\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 1.1883e-04\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 2s 34ms/step - loss: 1.1950e-04\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 1.1905e-04\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 1.1861e-04\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 1.1784e-04\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 1.1820e-04\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 1.1722e-04\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "31\n",
      "32\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "49/49 [==============================] - 5s 34ms/step - loss: 1.3292e-04\n",
      "Epoch 2/10\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 1.1924e-04\n",
      "Epoch 3/10\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 1.1844e-04\n",
      "Epoch 4/10\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 1.1871e-04\n",
      "Epoch 5/10\n",
      "49/49 [==============================] - 2s 33ms/step - loss: 1.1905e-04\n",
      "Epoch 6/10\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 1.1726e-04\n",
      "Epoch 7/10\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 1.1781e-04\n",
      "Epoch 8/10\n",
      "49/49 [==============================] - 1s 31ms/step - loss: 1.1606e-04\n",
      "Epoch 9/10\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 1.1729e-04\n",
      "Epoch 10/10\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 1.1608e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "32\n",
      "33\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "50/50 [==============================] - 6s 35ms/step - loss: 1.3370e-04\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 2s 34ms/step - loss: 1.1732e-04\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 2s 34ms/step - loss: 1.1582e-04\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 2s 34ms/step - loss: 1.1694e-04\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 2s 35ms/step - loss: 1.1663e-04\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 2s 36ms/step - loss: 1.1672e-04\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 2s 35ms/step - loss: 1.1597e-04\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 2s 37ms/step - loss: 1.1410e-04\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 2s 36ms/step - loss: 1.1555e-04\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 2s 36ms/step - loss: 1.1566e-04\n",
      "2/2 [==============================] - 1s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "33\n",
      "34\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "51/51 [==============================] - 6s 36ms/step - loss: 1.2567e-04\n",
      "Epoch 2/10\n",
      "51/51 [==============================] - 2s 36ms/step - loss: 1.1847e-04\n",
      "Epoch 3/10\n",
      "51/51 [==============================] - 2s 36ms/step - loss: 1.1867e-04\n",
      "Epoch 4/10\n",
      "51/51 [==============================] - 2s 36ms/step - loss: 1.1603e-04\n",
      "Epoch 5/10\n",
      "51/51 [==============================] - 2s 37ms/step - loss: 1.1650e-04\n",
      "Epoch 6/10\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 1.1620e-04\n",
      "Epoch 7/10\n",
      "51/51 [==============================] - 2s 38ms/step - loss: 1.1597e-04\n",
      "Epoch 8/10\n",
      "51/51 [==============================] - 2s 36ms/step - loss: 1.1458e-04\n",
      "Epoch 9/10\n",
      "51/51 [==============================] - 2s 37ms/step - loss: 1.1417e-04\n",
      "Epoch 10/10\n",
      "51/51 [==============================] - 2s 36ms/step - loss: 1.1420e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "34\n",
      "35\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "51/51 [==============================] - 6s 36ms/step - loss: 1.2585e-04\n",
      "Epoch 2/10\n",
      "51/51 [==============================] - 2s 35ms/step - loss: 1.1553e-04\n",
      "Epoch 3/10\n",
      "51/51 [==============================] - 2s 35ms/step - loss: 1.1441e-04\n",
      "Epoch 4/10\n",
      "51/51 [==============================] - 2s 36ms/step - loss: 1.1399e-04\n",
      "Epoch 5/10\n",
      "51/51 [==============================] - 2s 35ms/step - loss: 1.1590e-04\n",
      "Epoch 6/10\n",
      "51/51 [==============================] - 2s 36ms/step - loss: 1.1439e-04\n",
      "Epoch 7/10\n",
      "51/51 [==============================] - 2s 36ms/step - loss: 1.1405e-04\n",
      "Epoch 8/10\n",
      "51/51 [==============================] - 2s 35ms/step - loss: 1.1310e-04\n",
      "Epoch 9/10\n",
      "51/51 [==============================] - 2s 35ms/step - loss: 1.1265e-04\n",
      "Epoch 10/10\n",
      "51/51 [==============================] - 2s 35ms/step - loss: 1.1367e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "35\n",
      "36\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "52/52 [==============================] - 6s 35ms/step - loss: 1.2880e-04\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 2s 36ms/step - loss: 1.1323e-04\n",
      "Epoch 3/10\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 1.1464e-04\n",
      "Epoch 4/10\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 1.1369e-04\n",
      "Epoch 5/10\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 1.1397e-04\n",
      "Epoch 6/10\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 1.1384e-04\n",
      "Epoch 7/10\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 1.1310e-04\n",
      "Epoch 8/10\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 1.1214e-04\n",
      "Epoch 9/10\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 1.1304e-04\n",
      "Epoch 10/10\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 1.1200e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "36\n",
      "37\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "53/53 [==============================] - 6s 34ms/step - loss: 1.2569e-04\n",
      "Epoch 2/10\n",
      "53/53 [==============================] - 2s 34ms/step - loss: 1.1353e-04\n",
      "Epoch 3/10\n",
      "53/53 [==============================] - 2s 34ms/step - loss: 1.1408e-04\n",
      "Epoch 4/10\n",
      "53/53 [==============================] - 2s 36ms/step - loss: 1.1271e-04\n",
      "Epoch 5/10\n",
      "53/53 [==============================] - 2s 34ms/step - loss: 1.1341e-04\n",
      "Epoch 6/10\n",
      "53/53 [==============================] - 2s 35ms/step - loss: 1.1377e-04\n",
      "Epoch 7/10\n",
      "53/53 [==============================] - 2s 34ms/step - loss: 1.1224e-04\n",
      "Epoch 8/10\n",
      "53/53 [==============================] - 2s 34ms/step - loss: 1.1232e-04\n",
      "Epoch 9/10\n",
      "53/53 [==============================] - 2s 38ms/step - loss: 1.1202e-04\n",
      "Epoch 10/10\n",
      "53/53 [==============================] - 2s 35ms/step - loss: 1.1181e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "37\n",
      "38\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 6s 31ms/step - loss: 1.1995e-04\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 2s 31ms/step - loss: 1.1603e-04\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 2s 32ms/step - loss: 1.1401e-04\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 2s 32ms/step - loss: 1.1284e-04\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 2s 32ms/step - loss: 1.1198e-04\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 2s 33ms/step - loss: 1.1227e-04\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 2s 31ms/step - loss: 1.1013e-04\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 2s 33ms/step - loss: 1.1052e-04\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 2s 32ms/step - loss: 1.1133e-04\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 2s 31ms/step - loss: 1.1055e-04\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "38\n",
      "39\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "55/55 [==============================] - 5s 30ms/step - loss: 1.1694e-04\n",
      "Epoch 2/10\n",
      "55/55 [==============================] - 2s 29ms/step - loss: 1.1252e-04\n",
      "Epoch 3/10\n",
      "55/55 [==============================] - 2s 29ms/step - loss: 1.1347e-04\n",
      "Epoch 4/10\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1.1163e-04\n",
      "Epoch 5/10\n",
      "55/55 [==============================] - 2s 29ms/step - loss: 1.1062e-04\n",
      "Epoch 6/10\n",
      "55/55 [==============================] - 2s 30ms/step - loss: 1.1110e-04\n",
      "Epoch 7/10\n",
      "55/55 [==============================] - 2s 30ms/step - loss: 1.1029e-04\n",
      "Epoch 8/10\n",
      "55/55 [==============================] - 2s 30ms/step - loss: 1.0954e-04\n",
      "Epoch 9/10\n",
      "55/55 [==============================] - 2s 36ms/step - loss: 1.0867e-04\n",
      "Epoch 10/10\n",
      "55/55 [==============================] - 2s 41ms/step - loss: 1.0913e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 36ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "39\n",
      "40\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "55/55 [==============================] - 6s 34ms/step - loss: 1.2233e-04\n",
      "Epoch 2/10\n",
      "55/55 [==============================] - 2s 34ms/step - loss: 1.1084e-04\n",
      "Epoch 3/10\n",
      "55/55 [==============================] - 2s 34ms/step - loss: 1.0981e-04\n",
      "Epoch 4/10\n",
      "55/55 [==============================] - 2s 33ms/step - loss: 1.1002e-04\n",
      "Epoch 5/10\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1.0866e-04\n",
      "Epoch 6/10\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1.0878e-04\n",
      "Epoch 7/10\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1.0886e-04\n",
      "Epoch 8/10\n",
      "55/55 [==============================] - 2s 34ms/step - loss: 1.0841e-04\n",
      "Epoch 9/10\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1.0889e-04\n",
      "Epoch 10/10\n",
      "55/55 [==============================] - 2s 31ms/step - loss: 1.0876e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "40\n",
      "41\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "56/56 [==============================] - 6s 31ms/step - loss: 1.1862e-04\n",
      "Epoch 2/10\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 1.0987e-04\n",
      "Epoch 3/10\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 1.0937e-04\n",
      "Epoch 4/10\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 1.0881e-04\n",
      "Epoch 5/10\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 1.0825e-04\n",
      "Epoch 6/10\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 1.0844e-04\n",
      "Epoch 7/10\n",
      "56/56 [==============================] - 2s 36ms/step - loss: 1.0817e-04\n",
      "Epoch 8/10\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 1.0733e-04\n",
      "Epoch 9/10\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 1.0773e-04\n",
      "Epoch 10/10\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 1.0759e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "41\n",
      "42\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "57/57 [==============================] - 6s 33ms/step - loss: 1.2006e-04\n",
      "Epoch 2/10\n",
      "57/57 [==============================] - 2s 33ms/step - loss: 1.0970e-04\n",
      "Epoch 3/10\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 1.0940e-04\n",
      "Epoch 4/10\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 1.0762e-04\n",
      "Epoch 5/10\n",
      "57/57 [==============================] - 2s 38ms/step - loss: 1.0823e-04\n",
      "Epoch 6/10\n",
      "57/57 [==============================] - 2s 35ms/step - loss: 1.0808e-04\n",
      "Epoch 7/10\n",
      "57/57 [==============================] - 2s 34ms/step - loss: 1.0750e-04\n",
      "Epoch 8/10\n",
      "57/57 [==============================] - 2s 36ms/step - loss: 1.0719e-04\n",
      "Epoch 9/10\n",
      "57/57 [==============================] - 2s 32ms/step - loss: 1.0754e-04\n",
      "Epoch 10/10\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 1.0675e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "42\n",
      "43\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 6s 37ms/step - loss: 1.1942e-04\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 1.0933e-04\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 1.0894e-04\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 2s 31ms/step - loss: 1.1011e-04\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 2s 31ms/step - loss: 1.0898e-04\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 2s 31ms/step - loss: 1.0936e-04\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 1.0887e-04\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 1.0775e-04\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 1.0751e-04\n",
      "Epoch 10/10\n",
      "58/58 [==============================] - 2s 34ms/step - loss: 1.0758e-04\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "43\n",
      "44\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 7s 36ms/step - loss: 1.2434e-04\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 2s 39ms/step - loss: 1.1107e-04\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 2s 39ms/step - loss: 1.1079e-04\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 2s 37ms/step - loss: 1.1059e-04\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 2s 37ms/step - loss: 1.1077e-04\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 2s 37ms/step - loss: 1.1024e-04\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 2s 36ms/step - loss: 1.0916e-04\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - 2s 39ms/step - loss: 1.0924e-04\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - 2s 36ms/step - loss: 1.0820e-04\n",
      "Epoch 10/10\n",
      "58/58 [==============================] - 2s 37ms/step - loss: 1.0887e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "44\n",
      "45\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "59/59 [==============================] - 6s 34ms/step - loss: 1.1907e-04\n",
      "Epoch 2/10\n",
      "59/59 [==============================] - 2s 37ms/step - loss: 1.1277e-04\n",
      "Epoch 3/10\n",
      "59/59 [==============================] - 2s 34ms/step - loss: 1.1389e-04\n",
      "Epoch 4/10\n",
      "59/59 [==============================] - 2s 35ms/step - loss: 1.1173e-04\n",
      "Epoch 5/10\n",
      "59/59 [==============================] - 2s 35ms/step - loss: 1.1097e-04\n",
      "Epoch 6/10\n",
      "59/59 [==============================] - 2s 34ms/step - loss: 1.0986e-04\n",
      "Epoch 7/10\n",
      "59/59 [==============================] - 2s 34ms/step - loss: 1.1097e-04\n",
      "Epoch 8/10\n",
      "59/59 [==============================] - 2s 35ms/step - loss: 1.0965e-04\n",
      "Epoch 9/10\n",
      "59/59 [==============================] - 2s 36ms/step - loss: 1.0948e-04\n",
      "Epoch 10/10\n",
      "59/59 [==============================] - 2s 36ms/step - loss: 1.0938e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "45\n",
      "46\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 6s 37ms/step - loss: 1.1906e-04\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - 2s 37ms/step - loss: 1.1449e-04\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - 2s 37ms/step - loss: 1.1469e-04\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - 2s 37ms/step - loss: 1.1369e-04\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - 2s 37ms/step - loss: 1.1195e-04\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - 2s 37ms/step - loss: 1.1129e-04\n",
      "Epoch 7/10\n",
      "60/60 [==============================] - 2s 37ms/step - loss: 1.1135e-04\n",
      "Epoch 8/10\n",
      "60/60 [==============================] - 2s 37ms/step - loss: 1.1114e-04\n",
      "Epoch 9/10\n",
      "60/60 [==============================] - 2s 39ms/step - loss: 1.1029e-04\n",
      "Epoch 10/10\n",
      "60/60 [==============================] - 2s 38ms/step - loss: 1.1021e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "46\n",
      "47\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "61/61 [==============================] - 6s 35ms/step - loss: 1.2458e-04\n",
      "Epoch 2/10\n",
      "61/61 [==============================] - 2s 35ms/step - loss: 1.1450e-04\n",
      "Epoch 3/10\n",
      "61/61 [==============================] - 2s 40ms/step - loss: 1.1417e-04\n",
      "Epoch 4/10\n",
      "61/61 [==============================] - 2s 41ms/step - loss: 1.1263e-04\n",
      "Epoch 5/10\n",
      "61/61 [==============================] - 2s 40ms/step - loss: 1.1307e-04\n",
      "Epoch 6/10\n",
      "61/61 [==============================] - 3s 41ms/step - loss: 1.1242e-04\n",
      "Epoch 7/10\n",
      "61/61 [==============================] - 2s 38ms/step - loss: 1.1114e-04\n",
      "Epoch 8/10\n",
      "61/61 [==============================] - 2s 41ms/step - loss: 1.1182e-04\n",
      "Epoch 9/10\n",
      "61/61 [==============================] - 3s 41ms/step - loss: 1.1102e-04\n",
      "Epoch 10/10\n",
      "61/61 [==============================] - 2s 41ms/step - loss: 1.1143e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "47\n",
      "48\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 6s 40ms/step - loss: 1.2786e-04\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 3s 41ms/step - loss: 1.2964e-04\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 2s 39ms/step - loss: 1.3027e-04\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 3s 41ms/step - loss: 1.2973e-04\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 3s 40ms/step - loss: 1.3062e-04\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 3s 41ms/step - loss: 1.2817e-04\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 2s 37ms/step - loss: 1.3206e-04\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 2s 38ms/step - loss: 1.2716e-04\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 2s 39ms/step - loss: 1.2561e-04\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 2s 39ms/step - loss: 1.2687e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "48\n",
      "49\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 6s 38ms/step - loss: 1.5706e-04\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 2s 39ms/step - loss: 1.4524e-04\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 2s 40ms/step - loss: 1.4561e-04\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 2s 40ms/step - loss: 1.4568e-04\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 2s 39ms/step - loss: 1.4668e-04\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 2s 39ms/step - loss: 1.4560e-04\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 2s 39ms/step - loss: 1.4573e-04\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 3s 40ms/step - loss: 1.4496e-04\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 2s 40ms/step - loss: 1.4598e-04\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 2s 39ms/step - loss: 1.4485e-04\n",
      "2/2 [==============================] - 1s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "49\n",
      "50\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 7s 51ms/step - loss: 1.5670e-04\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 3s 45ms/step - loss: 1.5410e-04\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 3s 44ms/step - loss: 1.5166e-04\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 3s 45ms/step - loss: 1.4941e-04\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 3s 45ms/step - loss: 1.4945e-04\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 3s 45ms/step - loss: 1.4914e-04\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 3s 41ms/step - loss: 1.4857e-04\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 3s 40ms/step - loss: 1.4798e-04\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 3s 41ms/step - loss: 1.4843e-04\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 2s 40ms/step - loss: 1.5018e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "50\n",
      "51\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 7s 40ms/step - loss: 1.6256e-04\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 2s 37ms/step - loss: 1.6179e-04\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 3s 43ms/step - loss: 1.5998e-04\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 3s 47ms/step - loss: 1.5821e-04\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 3s 41ms/step - loss: 1.5810e-04\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 1.5621e-04\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.5444e-04\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.5652e-04\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 1.5538e-04\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 2s 36ms/step - loss: 1.5444e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "51\n",
      "52\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "65/65 [==============================] - 9s 45ms/step - loss: 1.6490e-04\n",
      "Epoch 2/10\n",
      "65/65 [==============================] - 3s 45ms/step - loss: 1.6008e-04\n",
      "Epoch 3/10\n",
      "65/65 [==============================] - 3s 41ms/step - loss: 1.5993e-04\n",
      "Epoch 4/10\n",
      "65/65 [==============================] - 3s 44ms/step - loss: 1.5766e-04\n",
      "Epoch 5/10\n",
      "65/65 [==============================] - 3s 48ms/step - loss: 1.5778e-04\n",
      "Epoch 6/10\n",
      "65/65 [==============================] - 3s 43ms/step - loss: 1.5678e-04\n",
      "Epoch 7/10\n",
      "65/65 [==============================] - 3s 44ms/step - loss: 1.5776e-04\n",
      "Epoch 8/10\n",
      "65/65 [==============================] - 3s 42ms/step - loss: 1.5622e-04\n",
      "Epoch 9/10\n",
      "65/65 [==============================] - 3s 42ms/step - loss: 1.5552e-04\n",
      "Epoch 10/10\n",
      "65/65 [==============================] - 3s 43ms/step - loss: 1.5455e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "52\n",
      "53\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "65/65 [==============================] - 7s 45ms/step - loss: 1.7280e-04\n",
      "Epoch 2/10\n",
      "65/65 [==============================] - 3s 44ms/step - loss: 1.5883e-04\n",
      "Epoch 3/10\n",
      "65/65 [==============================] - 3s 44ms/step - loss: 1.6143e-04\n",
      "Epoch 4/10\n",
      "65/65 [==============================] - 3s 45ms/step - loss: 1.6013e-04\n",
      "Epoch 5/10\n",
      "65/65 [==============================] - 3s 45ms/step - loss: 1.5783e-04\n",
      "Epoch 6/10\n",
      "65/65 [==============================] - 3s 45ms/step - loss: 1.5699e-04\n",
      "Epoch 7/10\n",
      "65/65 [==============================] - 3s 44ms/step - loss: 1.5692e-04\n",
      "Epoch 8/10\n",
      "65/65 [==============================] - 3s 44ms/step - loss: 1.5595e-04\n",
      "Epoch 9/10\n",
      "65/65 [==============================] - 3s 46ms/step - loss: 1.5557e-04\n",
      "Epoch 10/10\n",
      "65/65 [==============================] - 3s 47ms/step - loss: 1.5458e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "53\n",
      "54\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "66/66 [==============================] - 7s 42ms/step - loss: 1.6419e-04\n",
      "Epoch 2/10\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.6211e-04\n",
      "Epoch 3/10\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.6006e-04\n",
      "Epoch 4/10\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.5925e-04\n",
      "Epoch 5/10\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 1.5741e-04\n",
      "Epoch 6/10\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.5887e-04\n",
      "Epoch 7/10\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 1.5592e-04\n",
      "Epoch 8/10\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 1.5597e-04\n",
      "Epoch 9/10\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 1.5469e-04\n",
      "Epoch 10/10\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.5519e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "54\n",
      "55\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 7s 43ms/step - loss: 1.6539e-04\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 3s 46ms/step - loss: 1.5667e-04\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 1.5961e-04\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 1.5742e-04\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 1.5613e-04\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 1.5505e-04\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 1.5657e-04\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 1.5569e-04\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 3s 44ms/step - loss: 1.5506e-04\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 3s 43ms/step - loss: 1.5295e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "55\n",
      "56\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "68/68 [==============================] - 7s 42ms/step - loss: 1.6250e-04\n",
      "Epoch 2/10\n",
      "68/68 [==============================] - 3s 43ms/step - loss: 1.5775e-04\n",
      "Epoch 3/10\n",
      "68/68 [==============================] - 3s 43ms/step - loss: 1.5692e-04\n",
      "Epoch 4/10\n",
      "68/68 [==============================] - 3s 41ms/step - loss: 1.5492e-04\n",
      "Epoch 5/10\n",
      "68/68 [==============================] - 3s 41ms/step - loss: 1.5580e-04\n",
      "Epoch 6/10\n",
      "68/68 [==============================] - 3s 42ms/step - loss: 1.5431e-04\n",
      "Epoch 7/10\n",
      "68/68 [==============================] - 3s 41ms/step - loss: 1.5531e-04\n",
      "Epoch 8/10\n",
      "68/68 [==============================] - 3s 43ms/step - loss: 1.5615e-04\n",
      "Epoch 9/10\n",
      "68/68 [==============================] - 3s 42ms/step - loss: 1.5405e-04\n",
      "Epoch 10/10\n",
      "68/68 [==============================] - 3s 41ms/step - loss: 1.5273e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "56\n",
      "57\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "69/69 [==============================] - 7s 42ms/step - loss: 1.6315e-04\n",
      "Epoch 2/10\n",
      "69/69 [==============================] - 3s 38ms/step - loss: 1.5898e-04\n",
      "Epoch 3/10\n",
      "69/69 [==============================] - 3s 41ms/step - loss: 1.5842e-04\n",
      "Epoch 4/10\n",
      "69/69 [==============================] - 3s 39ms/step - loss: 1.5993e-04\n",
      "Epoch 5/10\n",
      "69/69 [==============================] - 3s 37ms/step - loss: 1.5671e-04\n",
      "Epoch 6/10\n",
      "69/69 [==============================] - 3s 37ms/step - loss: 1.5802e-04\n",
      "Epoch 7/10\n",
      "69/69 [==============================] - 3s 36ms/step - loss: 1.5594e-04\n",
      "Epoch 8/10\n",
      "69/69 [==============================] - 2s 36ms/step - loss: 1.5517e-04\n",
      "Epoch 9/10\n",
      "69/69 [==============================] - 3s 37ms/step - loss: 1.5542e-04\n",
      "Epoch 10/10\n",
      "69/69 [==============================] - 3s 37ms/step - loss: 1.5420e-04\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "57\n",
      "58\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "69/69 [==============================] - 7s 39ms/step - loss: 1.6471e-04\n",
      "Epoch 2/10\n",
      "69/69 [==============================] - 3s 40ms/step - loss: 1.5972e-04\n",
      "Epoch 3/10\n",
      "69/69 [==============================] - 3s 41ms/step - loss: 1.5960e-04\n",
      "Epoch 4/10\n",
      "69/69 [==============================] - 3s 40ms/step - loss: 1.5850e-04\n",
      "Epoch 5/10\n",
      "69/69 [==============================] - 3s 41ms/step - loss: 1.5780e-04\n",
      "Epoch 6/10\n",
      "69/69 [==============================] - 3s 40ms/step - loss: 1.5568e-04\n",
      "Epoch 7/10\n",
      "69/69 [==============================] - 3s 40ms/step - loss: 1.5528e-04\n",
      "Epoch 8/10\n",
      "69/69 [==============================] - 3s 40ms/step - loss: 1.5650e-04\n",
      "Epoch 9/10\n",
      "69/69 [==============================] - 3s 40ms/step - loss: 1.5561e-04\n",
      "Epoch 10/10\n",
      "69/69 [==============================] - 3s 41ms/step - loss: 1.5441e-04\n",
      "2/2 [==============================] - 1s 11ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "58\n",
      "59\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "70/70 [==============================] - 7s 39ms/step - loss: 1.6937e-04\n",
      "Epoch 2/10\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 1.5924e-04\n",
      "Epoch 3/10\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 1.5720e-04\n",
      "Epoch 4/10\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 1.5779e-04\n",
      "Epoch 5/10\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 1.5588e-04\n",
      "Epoch 6/10\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 1.5563e-04\n",
      "Epoch 7/10\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 1.5513e-04\n",
      "Epoch 8/10\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 1.5383e-04\n",
      "Epoch 9/10\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 1.5353e-04\n",
      "Epoch 10/10\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 1.5363e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "59\n",
      "60\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "71/71 [==============================] - 8s 47ms/step - loss: 1.6324e-04\n",
      "Epoch 2/10\n",
      "71/71 [==============================] - 4s 49ms/step - loss: 1.5504e-04\n",
      "Epoch 3/10\n",
      "71/71 [==============================] - 3s 46ms/step - loss: 1.5321e-04\n",
      "Epoch 4/10\n",
      "71/71 [==============================] - 3s 45ms/step - loss: 1.5381e-04\n",
      "Epoch 5/10\n",
      "71/71 [==============================] - 3s 47ms/step - loss: 1.5499e-04\n",
      "Epoch 6/10\n",
      "71/71 [==============================] - 3s 44ms/step - loss: 1.5388e-04\n",
      "Epoch 7/10\n",
      "71/71 [==============================] - 3s 45ms/step - loss: 1.5223e-04\n",
      "Epoch 8/10\n",
      "71/71 [==============================] - 3s 43ms/step - loss: 1.5347e-04\n",
      "Epoch 9/10\n",
      "71/71 [==============================] - 3s 45ms/step - loss: 1.5146e-04\n",
      "Epoch 10/10\n",
      "71/71 [==============================] - 3s 45ms/step - loss: 1.5080e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "60\n",
      "61\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 8s 48ms/step - loss: 1.5989e-04\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 1.5713e-04\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 1.5382e-04\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 1.5429e-04\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 3s 44ms/step - loss: 1.5352e-04\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 3s 45ms/step - loss: 1.5393e-04\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 3s 45ms/step - loss: 1.5286e-04\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 3s 45ms/step - loss: 1.5150e-04\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 3s 44ms/step - loss: 1.5264e-04\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 3s 44ms/step - loss: 1.5157e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "61\n",
      "62\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "73/73 [==============================] - 7s 44ms/step - loss: 1.5829e-04\n",
      "Epoch 2/10\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 1.5612e-04\n",
      "Epoch 3/10\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 1.5621e-04\n",
      "Epoch 4/10\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 1.5317e-04\n",
      "Epoch 5/10\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 1.5713e-04\n",
      "Epoch 6/10\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 1.5337e-04\n",
      "Epoch 7/10\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 1.5191e-04\n",
      "Epoch 8/10\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 1.5250e-04\n",
      "Epoch 9/10\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 1.5103e-04\n",
      "Epoch 10/10\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 1.4950e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "62\n",
      "63\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "73/73 [==============================] - 7s 44ms/step - loss: 1.6448e-04\n",
      "Epoch 2/10\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 1.5838e-04\n",
      "Epoch 3/10\n",
      "73/73 [==============================] - 3s 43ms/step - loss: 1.5907e-04\n",
      "Epoch 4/10\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 1.5728e-04\n",
      "Epoch 5/10\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 1.5809e-04\n",
      "Epoch 6/10\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 1.5666e-04\n",
      "Epoch 7/10\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 1.5635e-04\n",
      "Epoch 8/10\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 1.5590e-04\n",
      "Epoch 9/10\n",
      "73/73 [==============================] - 3s 45ms/step - loss: 1.5454e-04\n",
      "Epoch 10/10\n",
      "73/73 [==============================] - 3s 44ms/step - loss: 1.5413e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 34ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "63\n",
      "64\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "74/74 [==============================] - 8s 45ms/step - loss: 1.6460e-04\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 3s 45ms/step - loss: 1.5964e-04\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 3s 45ms/step - loss: 1.5934e-04\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 3s 45ms/step - loss: 1.6084e-04\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 3s 47ms/step - loss: 1.5769e-04\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 3s 45ms/step - loss: 1.5798e-04\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 3s 45ms/step - loss: 1.5718e-04\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 3s 45ms/step - loss: 1.5769e-04\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 3s 45ms/step - loss: 1.5682e-04\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 3s 45ms/step - loss: 1.5657e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "64\n",
      "65\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "75/75 [==============================] - 7s 41ms/step - loss: 1.6533e-04\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 3s 42ms/step - loss: 1.6203e-04\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 3s 41ms/step - loss: 1.6015e-04\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 3s 41ms/step - loss: 1.5852e-04\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 3s 42ms/step - loss: 1.5781e-04\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 3s 42ms/step - loss: 1.5835e-04\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 3s 41ms/step - loss: 1.5771e-04\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 3s 43ms/step - loss: 1.5746e-04\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 3s 42ms/step - loss: 1.5743e-04\n",
      "Epoch 10/10\n",
      "75/75 [==============================] - 3s 42ms/step - loss: 1.5486e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "65\n",
      "66\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "76/76 [==============================] - 7s 39ms/step - loss: 1.6682e-04\n",
      "Epoch 2/10\n",
      "76/76 [==============================] - 3s 39ms/step - loss: 1.5601e-04\n",
      "Epoch 3/10\n",
      "76/76 [==============================] - 3s 39ms/step - loss: 1.5993e-04\n",
      "Epoch 4/10\n",
      "76/76 [==============================] - 3s 39ms/step - loss: 1.5819e-04\n",
      "Epoch 5/10\n",
      "76/76 [==============================] - 3s 39ms/step - loss: 1.5586e-04\n",
      "Epoch 6/10\n",
      "76/76 [==============================] - 3s 38ms/step - loss: 1.5552e-04\n",
      "Epoch 7/10\n",
      "76/76 [==============================] - 3s 38ms/step - loss: 1.5556e-04\n",
      "Epoch 8/10\n",
      "76/76 [==============================] - 3s 38ms/step - loss: 1.5542e-04\n",
      "Epoch 9/10\n",
      "76/76 [==============================] - 3s 38ms/step - loss: 1.5422e-04\n",
      "Epoch 10/10\n",
      "76/76 [==============================] - 3s 38ms/step - loss: 1.5418e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "66\n",
      "67\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "76/76 [==============================] - 7s 40ms/step - loss: 1.6520e-04\n",
      "Epoch 2/10\n",
      "76/76 [==============================] - 3s 39ms/step - loss: 1.5802e-04\n",
      "Epoch 3/10\n",
      "76/76 [==============================] - 3s 39ms/step - loss: 1.5939e-04\n",
      "Epoch 4/10\n",
      "76/76 [==============================] - 3s 39ms/step - loss: 1.5893e-04\n",
      "Epoch 5/10\n",
      "76/76 [==============================] - 3s 39ms/step - loss: 1.5721e-04\n",
      "Epoch 6/10\n",
      "76/76 [==============================] - 3s 39ms/step - loss: 1.5665e-04\n",
      "Epoch 7/10\n",
      "76/76 [==============================] - 3s 40ms/step - loss: 1.5609e-04\n",
      "Epoch 8/10\n",
      "76/76 [==============================] - 3s 40ms/step - loss: 1.5559e-04\n",
      "Epoch 9/10\n",
      "76/76 [==============================] - 3s 41ms/step - loss: 1.5490e-04\n",
      "Epoch 10/10\n",
      "76/76 [==============================] - 3s 39ms/step - loss: 1.5482e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 14ms/step\n",
      "67\n",
      "68\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "77/77 [==============================] - 7s 43ms/step - loss: 1.6333e-04\n",
      "Epoch 2/10\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 1.5769e-04\n",
      "Epoch 3/10\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 1.5585e-04\n",
      "Epoch 4/10\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 1.5595e-04\n",
      "Epoch 5/10\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 1.5439e-04\n",
      "Epoch 6/10\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 1.5448e-04\n",
      "Epoch 7/10\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 1.5367e-04\n",
      "Epoch 8/10\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 1.5250e-04\n",
      "Epoch 9/10\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 1.5198e-04\n",
      "Epoch 10/10\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 1.5230e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "68\n",
      "69\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "78/78 [==============================] - 7s 42ms/step - loss: 1.6767e-04\n",
      "Epoch 2/10\n",
      "78/78 [==============================] - 3s 43ms/step - loss: 1.5758e-04\n",
      "Epoch 3/10\n",
      "78/78 [==============================] - 3s 43ms/step - loss: 1.5742e-04\n",
      "Epoch 4/10\n",
      "78/78 [==============================] - 3s 43ms/step - loss: 1.5504e-04\n",
      "Epoch 5/10\n",
      "78/78 [==============================] - 3s 43ms/step - loss: 1.5491e-04\n",
      "Epoch 6/10\n",
      "78/78 [==============================] - 3s 43ms/step - loss: 1.5372e-04\n",
      "Epoch 7/10\n",
      "78/78 [==============================] - 3s 44ms/step - loss: 1.5339e-04\n",
      "Epoch 8/10\n",
      "78/78 [==============================] - 3s 43ms/step - loss: 1.5238e-04\n",
      "Epoch 9/10\n",
      "78/78 [==============================] - 3s 44ms/step - loss: 1.5186e-04\n",
      "Epoch 10/10\n",
      "78/78 [==============================] - 3s 43ms/step - loss: 1.5179e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "69\n",
      "70\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "79/79 [==============================] - 8s 47ms/step - loss: 1.6285e-04\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 4s 47ms/step - loss: 1.5555e-04\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 4s 49ms/step - loss: 1.5481e-04\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 4s 48ms/step - loss: 1.5347e-04\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 4s 47ms/step - loss: 1.5346e-04\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 4s 49ms/step - loss: 1.5219e-04\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 4s 48ms/step - loss: 1.5222e-04\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 4s 49ms/step - loss: 1.5119e-04\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 4s 48ms/step - loss: 1.5006e-04\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 4s 49ms/step - loss: 1.5053e-04\n",
      "2/2 [==============================] - 1s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "70\n",
      "71\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 8s 50ms/step - loss: 1.5967e-04\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 4s 51ms/step - loss: 1.5879e-04\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 4s 49ms/step - loss: 1.5284e-04\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 4s 50ms/step - loss: 1.5340e-04\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 4s 49ms/step - loss: 1.5305e-04\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 4s 50ms/step - loss: 1.5057e-04\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 4s 49ms/step - loss: 1.5163e-04\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 4s 49ms/step - loss: 1.5018e-04\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 4s 50ms/step - loss: 1.5005e-04\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 4s 50ms/step - loss: 1.4856e-04\n",
      "2/2 [==============================] - 1s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "71\n",
      "72\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 8s 53ms/step - loss: 1.6125e-04\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 1.5365e-04\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 1.5325e-04\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 4s 55ms/step - loss: 1.5224e-04\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 1.5211e-04\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 1.5285e-04\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 1.5089e-04\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 1.5045e-04\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 4s 53ms/step - loss: 1.4974e-04\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 4s 54ms/step - loss: 1.4866e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "72\n",
      "73\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "81/81 [==============================] - 9s 54ms/step - loss: 1.5832e-04\n",
      "Epoch 2/10\n",
      "81/81 [==============================] - 4s 54ms/step - loss: 1.5427e-04\n",
      "Epoch 3/10\n",
      "81/81 [==============================] - 4s 54ms/step - loss: 1.5245e-04\n",
      "Epoch 4/10\n",
      "81/81 [==============================] - 4s 55ms/step - loss: 1.5274e-04\n",
      "Epoch 5/10\n",
      "81/81 [==============================] - 4s 55ms/step - loss: 1.5011e-04\n",
      "Epoch 6/10\n",
      "81/81 [==============================] - 4s 54ms/step - loss: 1.4979e-04\n",
      "Epoch 7/10\n",
      "81/81 [==============================] - 4s 55ms/step - loss: 1.4916e-04\n",
      "Epoch 8/10\n",
      "81/81 [==============================] - 4s 55ms/step - loss: 1.4858e-04\n",
      "Epoch 9/10\n",
      "81/81 [==============================] - 4s 54ms/step - loss: 1.4871e-04\n",
      "Epoch 10/10\n",
      "81/81 [==============================] - 4s 54ms/step - loss: 1.4809e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "73\n",
      "74\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "82/82 [==============================] - 8s 46ms/step - loss: 1.6052e-04\n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 4s 45ms/step - loss: 1.5181e-04\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 4s 45ms/step - loss: 1.5161e-04\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 4s 45ms/step - loss: 1.5123e-04\n",
      "Epoch 5/10\n",
      "82/82 [==============================] - 4s 45ms/step - loss: 1.5082e-04\n",
      "Epoch 6/10\n",
      "82/82 [==============================] - 4s 45ms/step - loss: 1.4904e-04\n",
      "Epoch 7/10\n",
      "82/82 [==============================] - 4s 45ms/step - loss: 1.4880e-04\n",
      "Epoch 8/10\n",
      "82/82 [==============================] - 4s 45ms/step - loss: 1.4873e-04\n",
      "Epoch 9/10\n",
      "82/82 [==============================] - 4s 46ms/step - loss: 1.4778e-04\n",
      "Epoch 10/10\n",
      "82/82 [==============================] - 4s 45ms/step - loss: 1.4703e-04\n",
      "2/2 [==============================] - 1s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "74\n",
      "75\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "83/83 [==============================] - 8s 43ms/step - loss: 1.6024e-04\n",
      "Epoch 2/10\n",
      "83/83 [==============================] - 4s 43ms/step - loss: 1.5098e-04\n",
      "Epoch 3/10\n",
      "83/83 [==============================] - 4s 43ms/step - loss: 1.5034e-04\n",
      "Epoch 4/10\n",
      "83/83 [==============================] - 4s 44ms/step - loss: 1.4874e-04\n",
      "Epoch 5/10\n",
      "83/83 [==============================] - 4s 43ms/step - loss: 1.4964e-04\n",
      "Epoch 6/10\n",
      "83/83 [==============================] - 4s 44ms/step - loss: 1.4927e-04\n",
      "Epoch 7/10\n",
      "83/83 [==============================] - 4s 43ms/step - loss: 1.4765e-04\n",
      "Epoch 8/10\n",
      "83/83 [==============================] - 4s 43ms/step - loss: 1.4767e-04\n",
      "Epoch 9/10\n",
      "83/83 [==============================] - 4s 44ms/step - loss: 1.4665e-04\n",
      "Epoch 10/10\n",
      "83/83 [==============================] - 4s 43ms/step - loss: 1.4603e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 33ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "75\n",
      "76\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "83/83 [==============================] - 7s 43ms/step - loss: 1.5511e-04\n",
      "Epoch 2/10\n",
      "83/83 [==============================] - 4s 43ms/step - loss: 1.4967e-04\n",
      "Epoch 3/10\n",
      "83/83 [==============================] - 4s 43ms/step - loss: 1.4851e-04\n",
      "Epoch 4/10\n",
      "83/83 [==============================] - 4s 44ms/step - loss: 1.4798e-04\n",
      "Epoch 5/10\n",
      "83/83 [==============================] - 4s 43ms/step - loss: 1.4865e-04\n",
      "Epoch 6/10\n",
      "83/83 [==============================] - 4s 43ms/step - loss: 1.4678e-04\n",
      "Epoch 7/10\n",
      "83/83 [==============================] - 4s 43ms/step - loss: 1.4756e-04\n",
      "Epoch 8/10\n",
      "83/83 [==============================] - 3s 42ms/step - loss: 1.4625e-04\n",
      "Epoch 9/10\n",
      "83/83 [==============================] - 4s 44ms/step - loss: 1.4576e-04\n",
      "Epoch 10/10\n",
      "83/83 [==============================] - 4s 43ms/step - loss: 1.4497e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 35ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "76\n",
      "77\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "84/84 [==============================] - 8s 47ms/step - loss: 1.5475e-04\n",
      "Epoch 2/10\n",
      "84/84 [==============================] - 4s 47ms/step - loss: 1.5116e-04\n",
      "Epoch 3/10\n",
      "84/84 [==============================] - 4s 48ms/step - loss: 1.4911e-04\n",
      "Epoch 4/10\n",
      "84/84 [==============================] - 4s 47ms/step - loss: 1.4864e-04\n",
      "Epoch 5/10\n",
      "84/84 [==============================] - 4s 47ms/step - loss: 1.4635e-04\n",
      "Epoch 6/10\n",
      "84/84 [==============================] - 4s 48ms/step - loss: 1.4600e-04\n",
      "Epoch 7/10\n",
      "84/84 [==============================] - 4s 47ms/step - loss: 1.4586e-04\n",
      "Epoch 8/10\n",
      "84/84 [==============================] - 4s 48ms/step - loss: 1.4539e-04\n",
      "Epoch 9/10\n",
      "84/84 [==============================] - 4s 49ms/step - loss: 1.4527e-04\n",
      "Epoch 10/10\n",
      "84/84 [==============================] - 4s 47ms/step - loss: 1.4441e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "77\n",
      "78\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "85/85 [==============================] - 9s 51ms/step - loss: 1.5282e-04\n",
      "Epoch 2/10\n",
      "85/85 [==============================] - 4s 51ms/step - loss: 1.4930e-04\n",
      "Epoch 3/10\n",
      "85/85 [==============================] - 4s 51ms/step - loss: 1.4777e-04\n",
      "Epoch 4/10\n",
      "85/85 [==============================] - 4s 50ms/step - loss: 1.4609e-04\n",
      "Epoch 5/10\n",
      "85/85 [==============================] - 4s 50ms/step - loss: 1.4733e-04\n",
      "Epoch 6/10\n",
      "85/85 [==============================] - 4s 51ms/step - loss: 1.4544e-04\n",
      "Epoch 7/10\n",
      "85/85 [==============================] - 4s 50ms/step - loss: 1.4436e-04\n",
      "Epoch 8/10\n",
      "85/85 [==============================] - 4s 50ms/step - loss: 1.4441e-04\n",
      "Epoch 9/10\n",
      "85/85 [==============================] - 4s 50ms/step - loss: 1.4370e-04\n",
      "Epoch 10/10\n",
      "85/85 [==============================] - 4s 51ms/step - loss: 1.4334e-04\n",
      "2/2 [==============================] - 1s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "78\n",
      "79\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 9s 51ms/step - loss: 1.5274e-04\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 4s 52ms/step - loss: 1.4879e-04\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 4s 51ms/step - loss: 1.4890e-04\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 4s 52ms/step - loss: 1.4533e-04\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 5s 54ms/step - loss: 1.4585e-04\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 5s 52ms/step - loss: 1.4500e-04\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 5s 53ms/step - loss: 1.4434e-04\n",
      "Epoch 8/10\n",
      "86/86 [==============================] - 5s 52ms/step - loss: 1.4325e-04\n",
      "Epoch 9/10\n",
      "86/86 [==============================] - 5s 53ms/step - loss: 1.4280e-04\n",
      "Epoch 10/10\n",
      "86/86 [==============================] - 5s 53ms/step - loss: 1.4278e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "79\n",
      "80\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "87/87 [==============================] - 9s 49ms/step - loss: 1.5072e-04\n",
      "Epoch 2/10\n",
      "87/87 [==============================] - 4s 49ms/step - loss: 1.4645e-04\n",
      "Epoch 3/10\n",
      "87/87 [==============================] - 4s 48ms/step - loss: 1.4681e-04\n",
      "Epoch 4/10\n",
      "87/87 [==============================] - 4s 49ms/step - loss: 1.4448e-04\n",
      "Epoch 5/10\n",
      "87/87 [==============================] - 4s 49ms/step - loss: 1.4542e-04\n",
      "Epoch 6/10\n",
      "87/87 [==============================] - 4s 49ms/step - loss: 1.4412e-04\n",
      "Epoch 7/10\n",
      "87/87 [==============================] - 4s 48ms/step - loss: 1.4272e-04\n",
      "Epoch 8/10\n",
      "87/87 [==============================] - 4s 49ms/step - loss: 1.4252e-04\n",
      "Epoch 9/10\n",
      "87/87 [==============================] - 4s 49ms/step - loss: 1.4198e-04\n",
      "Epoch 10/10\n",
      "87/87 [==============================] - 4s 51ms/step - loss: 1.4187e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "80\n",
      "81\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "87/87 [==============================] - 8s 45ms/step - loss: 1.5016e-04\n",
      "Epoch 2/10\n",
      "87/87 [==============================] - 4s 45ms/step - loss: 1.4443e-04\n",
      "Epoch 3/10\n",
      "87/87 [==============================] - 4s 44ms/step - loss: 1.4311e-04\n",
      "Epoch 4/10\n",
      "87/87 [==============================] - 4s 45ms/step - loss: 1.4377e-04\n",
      "Epoch 5/10\n",
      "87/87 [==============================] - 4s 45ms/step - loss: 1.4348e-04\n",
      "Epoch 6/10\n",
      "87/87 [==============================] - 4s 45ms/step - loss: 1.4208e-04\n",
      "Epoch 7/10\n",
      "87/87 [==============================] - 4s 44ms/step - loss: 1.4191e-04\n",
      "Epoch 8/10\n",
      "87/87 [==============================] - 4s 45ms/step - loss: 1.4141e-04\n",
      "Epoch 9/10\n",
      "87/87 [==============================] - 4s 44ms/step - loss: 1.4049e-04\n",
      "Epoch 10/10\n",
      "87/87 [==============================] - 4s 45ms/step - loss: 1.4017e-04\n",
      "2/2 [==============================] - 1s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "81\n",
      "82\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "88/88 [==============================] - 8s 43ms/step - loss: 1.4899e-04\n",
      "Epoch 2/10\n",
      "88/88 [==============================] - 4s 43ms/step - loss: 1.4544e-04\n",
      "Epoch 3/10\n",
      "88/88 [==============================] - 4s 43ms/step - loss: 1.4430e-04\n",
      "Epoch 4/10\n",
      "88/88 [==============================] - 4s 44ms/step - loss: 1.4282e-04\n",
      "Epoch 5/10\n",
      "88/88 [==============================] - 4s 43ms/step - loss: 1.4231e-04\n",
      "Epoch 6/10\n",
      "88/88 [==============================] - 4s 44ms/step - loss: 1.4227e-04\n",
      "Epoch 7/10\n",
      "88/88 [==============================] - 4s 43ms/step - loss: 1.4188e-04\n",
      "Epoch 8/10\n",
      "88/88 [==============================] - 4s 44ms/step - loss: 1.4054e-04\n",
      "Epoch 9/10\n",
      "88/88 [==============================] - 4s 43ms/step - loss: 1.4038e-04\n",
      "Epoch 10/10\n",
      "88/88 [==============================] - 4s 43ms/step - loss: 1.4016e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "82\n",
      "83\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "89/89 [==============================] - 9s 49ms/step - loss: 1.5090e-04\n",
      "Epoch 2/10\n",
      "89/89 [==============================] - 4s 49ms/step - loss: 1.4519e-04\n",
      "Epoch 3/10\n",
      "89/89 [==============================] - 4s 50ms/step - loss: 1.4311e-04\n",
      "Epoch 4/10\n",
      "89/89 [==============================] - 4s 49ms/step - loss: 1.4349e-04\n",
      "Epoch 5/10\n",
      "89/89 [==============================] - 4s 49ms/step - loss: 1.4372e-04\n",
      "Epoch 6/10\n",
      "89/89 [==============================] - 4s 49ms/step - loss: 1.4292e-04\n",
      "Epoch 7/10\n",
      "89/89 [==============================] - 4s 49ms/step - loss: 1.4263e-04\n",
      "Epoch 8/10\n",
      "89/89 [==============================] - 4s 50ms/step - loss: 1.4170e-04\n",
      "Epoch 9/10\n",
      "89/89 [==============================] - 4s 49ms/step - loss: 1.4122e-04\n",
      "Epoch 10/10\n",
      "89/89 [==============================] - 4s 51ms/step - loss: 1.4078e-04\n",
      "2/2 [==============================] - 1s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "83\n",
      "84\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "90/90 [==============================] - 9s 56ms/step - loss: 1.5107e-04\n",
      "Epoch 2/10\n",
      "90/90 [==============================] - 5s 56ms/step - loss: 1.4552e-04\n",
      "Epoch 3/10\n",
      "90/90 [==============================] - 5s 58ms/step - loss: 1.4354e-04\n",
      "Epoch 4/10\n",
      "90/90 [==============================] - 5s 56ms/step - loss: 1.4364e-04\n",
      "Epoch 5/10\n",
      "90/90 [==============================] - 5s 57ms/step - loss: 1.4344e-04\n",
      "Epoch 6/10\n",
      "90/90 [==============================] - 5s 57ms/step - loss: 1.4275e-04\n",
      "Epoch 7/10\n",
      "90/90 [==============================] - 5s 56ms/step - loss: 1.4133e-04\n",
      "Epoch 8/10\n",
      "90/90 [==============================] - 5s 57ms/step - loss: 1.4112e-04\n",
      "Epoch 9/10\n",
      "90/90 [==============================] - 5s 56ms/step - loss: 1.4057e-04\n",
      "Epoch 10/10\n",
      "90/90 [==============================] - 5s 57ms/step - loss: 1.4001e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "84\n",
      "85\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "90/90 [==============================] - 9s 53ms/step - loss: 1.5333e-04\n",
      "Epoch 2/10\n",
      "90/90 [==============================] - 5s 51ms/step - loss: 1.4326e-04\n",
      "Epoch 3/10\n",
      "90/90 [==============================] - 5s 53ms/step - loss: 1.4415e-04\n",
      "Epoch 4/10\n",
      "90/90 [==============================] - 5s 52ms/step - loss: 1.4369e-04\n",
      "Epoch 5/10\n",
      "90/90 [==============================] - 5s 52ms/step - loss: 1.4324e-04\n",
      "Epoch 6/10\n",
      "90/90 [==============================] - 5s 52ms/step - loss: 1.4237e-04\n",
      "Epoch 7/10\n",
      "90/90 [==============================] - 5s 52ms/step - loss: 1.4135e-04\n",
      "Epoch 8/10\n",
      "90/90 [==============================] - 5s 53ms/step - loss: 1.4084e-04\n",
      "Epoch 9/10\n",
      "90/90 [==============================] - 5s 53ms/step - loss: 1.4115e-04\n",
      "Epoch 10/10\n",
      "90/90 [==============================] - 5s 53ms/step - loss: 1.4020e-04\n",
      "2/2 [==============================] - 1s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "85\n",
      "86\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "91/91 [==============================] - 9s 50ms/step - loss: 1.5191e-04\n",
      "Epoch 2/10\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 1.4385e-04\n",
      "Epoch 3/10\n",
      "91/91 [==============================] - 5s 50ms/step - loss: 1.4475e-04\n",
      "Epoch 4/10\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 1.4253e-04\n",
      "Epoch 5/10\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 1.4248e-04\n",
      "Epoch 6/10\n",
      "91/91 [==============================] - 4s 48ms/step - loss: 1.4146e-04\n",
      "Epoch 7/10\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 1.4061e-04\n",
      "Epoch 8/10\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 1.3967e-04\n",
      "Epoch 9/10\n",
      "91/91 [==============================] - 4s 49ms/step - loss: 1.3998e-04\n",
      "Epoch 10/10\n",
      "91/91 [==============================] - 5s 49ms/step - loss: 1.3956e-04\n",
      "2/2 [==============================] - 1s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "86\n",
      "87\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "92/92 [==============================] - 8s 46ms/step - loss: 1.4816e-04\n",
      "Epoch 2/10\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 1.4485e-04\n",
      "Epoch 3/10\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 1.4239e-04\n",
      "Epoch 4/10\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 1.4197e-04\n",
      "Epoch 5/10\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 1.4195e-04\n",
      "Epoch 6/10\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 1.4044e-04\n",
      "Epoch 7/10\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 1.4094e-04\n",
      "Epoch 8/10\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 1.4024e-04\n",
      "Epoch 9/10\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 1.3910e-04\n",
      "Epoch 10/10\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 1.3916e-04\n",
      "2/2 [==============================] - 1s 10ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "87\n",
      "88\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "93/93 [==============================] - 8s 39ms/step - loss: 1.5124e-04\n",
      "Epoch 2/10\n",
      "93/93 [==============================] - 4s 39ms/step - loss: 1.4384e-04\n",
      "Epoch 3/10\n",
      "93/93 [==============================] - 4s 40ms/step - loss: 1.4202e-04\n",
      "Epoch 4/10\n",
      "93/93 [==============================] - 4s 45ms/step - loss: 1.4095e-04\n",
      "Epoch 5/10\n",
      "93/93 [==============================] - 4s 41ms/step - loss: 1.4164e-04\n",
      "Epoch 6/10\n",
      "93/93 [==============================] - 4s 42ms/step - loss: 1.4087e-04\n",
      "Epoch 7/10\n",
      "93/93 [==============================] - 4s 41ms/step - loss: 1.3988e-04\n",
      "Epoch 8/10\n",
      "93/93 [==============================] - 4s 41ms/step - loss: 1.3931e-04\n",
      "Epoch 9/10\n",
      "93/93 [==============================] - 4s 42ms/step - loss: 1.3848e-04\n",
      "Epoch 10/10\n",
      "93/93 [==============================] - 4s 43ms/step - loss: 1.3842e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "88\n",
      "89\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "94/94 [==============================] - 8s 45ms/step - loss: 1.4744e-04\n",
      "Epoch 2/10\n",
      "94/94 [==============================] - 4s 47ms/step - loss: 1.4236e-04\n",
      "Epoch 3/10\n",
      "94/94 [==============================] - 4s 48ms/step - loss: 1.4216e-04\n",
      "Epoch 4/10\n",
      "94/94 [==============================] - 4s 47ms/step - loss: 1.4111e-04\n",
      "Epoch 5/10\n",
      "94/94 [==============================] - 4s 47ms/step - loss: 1.4018e-04\n",
      "Epoch 6/10\n",
      "94/94 [==============================] - 4s 47ms/step - loss: 1.3995e-04\n",
      "Epoch 7/10\n",
      "94/94 [==============================] - 5s 49ms/step - loss: 1.3936e-04\n",
      "Epoch 8/10\n",
      "94/94 [==============================] - 4s 48ms/step - loss: 1.3858e-04\n",
      "Epoch 9/10\n",
      "94/94 [==============================] - 5s 48ms/step - loss: 1.3794e-04\n",
      "Epoch 10/10\n",
      "94/94 [==============================] - 4s 47ms/step - loss: 1.3779e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "89\n",
      "90\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "94/94 [==============================] - 8s 48ms/step - loss: 1.4750e-04\n",
      "Epoch 2/10\n",
      "94/94 [==============================] - 5s 48ms/step - loss: 1.4073e-04\n",
      "Epoch 3/10\n",
      "94/94 [==============================] - 4s 47ms/step - loss: 1.4022e-04\n",
      "Epoch 4/10\n",
      "94/94 [==============================] - 4s 47ms/step - loss: 1.4007e-04\n",
      "Epoch 5/10\n",
      "94/94 [==============================] - 4s 47ms/step - loss: 1.3910e-04\n",
      "Epoch 6/10\n",
      "94/94 [==============================] - 4s 47ms/step - loss: 1.3956e-04\n",
      "Epoch 7/10\n",
      "94/94 [==============================] - 4s 47ms/step - loss: 1.3780e-04\n",
      "Epoch 8/10\n",
      "94/94 [==============================] - 4s 47ms/step - loss: 1.3668e-04\n",
      "Epoch 9/10\n",
      "94/94 [==============================] - 4s 48ms/step - loss: 1.3681e-04\n",
      "Epoch 10/10\n",
      "94/94 [==============================] - 4s 47ms/step - loss: 1.3659e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "90\n",
      "91\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "95/95 [==============================] - 9s 54ms/step - loss: 1.4709e-04\n",
      "Epoch 2/10\n",
      "95/95 [==============================] - 5s 53ms/step - loss: 1.4023e-04\n",
      "Epoch 3/10\n",
      "95/95 [==============================] - 5s 54ms/step - loss: 1.3939e-04\n",
      "Epoch 4/10\n",
      "95/95 [==============================] - 5s 55ms/step - loss: 1.3884e-04\n",
      "Epoch 5/10\n",
      "95/95 [==============================] - 5s 54ms/step - loss: 1.3796e-04\n",
      "Epoch 6/10\n",
      "95/95 [==============================] - 5s 54ms/step - loss: 1.3745e-04\n",
      "Epoch 7/10\n",
      "95/95 [==============================] - 5s 53ms/step - loss: 1.3731e-04\n",
      "Epoch 8/10\n",
      "95/95 [==============================] - 5s 54ms/step - loss: 1.3637e-04\n",
      "Epoch 9/10\n",
      "95/95 [==============================] - 5s 55ms/step - loss: 1.3588e-04\n",
      "Epoch 10/10\n",
      "95/95 [==============================] - 5s 54ms/step - loss: 1.3572e-04\n",
      "2/2 [==============================] - 1s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "91\n",
      "92\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "96/96 [==============================] - 10s 54ms/step - loss: 1.4368e-04\n",
      "Epoch 2/10\n",
      "96/96 [==============================] - 5s 53ms/step - loss: 1.3902e-04\n",
      "Epoch 3/10\n",
      "96/96 [==============================] - 5s 54ms/step - loss: 1.3788e-04\n",
      "Epoch 4/10\n",
      "96/96 [==============================] - 5s 54ms/step - loss: 1.3781e-04\n",
      "Epoch 5/10\n",
      "96/96 [==============================] - 5s 53ms/step - loss: 1.3714e-04\n",
      "Epoch 6/10\n",
      "96/96 [==============================] - 5s 54ms/step - loss: 1.3623e-04\n",
      "Epoch 7/10\n",
      "96/96 [==============================] - 5s 54ms/step - loss: 1.3646e-04\n",
      "Epoch 8/10\n",
      "96/96 [==============================] - 5s 54ms/step - loss: 1.3519e-04\n",
      "Epoch 9/10\n",
      "96/96 [==============================] - 5s 54ms/step - loss: 1.3480e-04\n",
      "Epoch 10/10\n",
      "96/96 [==============================] - 5s 54ms/step - loss: 1.3426e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "92\n",
      "93\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "97/97 [==============================] - 9s 51ms/step - loss: 1.4263e-04\n",
      "Epoch 2/10\n",
      "97/97 [==============================] - 5s 51ms/step - loss: 1.3833e-04\n",
      "Epoch 3/10\n",
      "97/97 [==============================] - 5s 50ms/step - loss: 1.3715e-04\n",
      "Epoch 4/10\n",
      "97/97 [==============================] - 5s 51ms/step - loss: 1.3596e-04\n",
      "Epoch 5/10\n",
      "97/97 [==============================] - 5s 52ms/step - loss: 1.3625e-04\n",
      "Epoch 6/10\n",
      "97/97 [==============================] - 5s 50ms/step - loss: 1.3568e-04\n",
      "Epoch 7/10\n",
      "97/97 [==============================] - 5s 51ms/step - loss: 1.3471e-04\n",
      "Epoch 8/10\n",
      "97/97 [==============================] - 5s 52ms/step - loss: 1.3454e-04\n",
      "Epoch 9/10\n",
      "97/97 [==============================] - 5s 52ms/step - loss: 1.3336e-04\n",
      "Epoch 10/10\n",
      "97/97 [==============================] - 5s 51ms/step - loss: 1.3359e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "93\n",
      "94\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "98/98 [==============================] - 10s 56ms/step - loss: 1.4489e-04\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 5s 55ms/step - loss: 1.3619e-04\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 5s 55ms/step - loss: 1.3675e-04\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 5s 55ms/step - loss: 1.3530e-04\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 5s 55ms/step - loss: 1.3481e-04\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 6s 57ms/step - loss: 1.3498e-04\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 6s 56ms/step - loss: 1.3391e-04\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 6s 56ms/step - loss: 1.3371e-04\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 6s 56ms/step - loss: 1.3219e-04\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 6s 57ms/step - loss: 1.3248e-04\n",
      "2/2 [==============================] - 1s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "94\n",
      "95\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "98/98 [==============================] - 9s 53ms/step - loss: 1.4134e-04\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 5s 52ms/step - loss: 1.3854e-04\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 5s 53ms/step - loss: 1.3525e-04\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 5s 52ms/step - loss: 1.3533e-04\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 5s 52ms/step - loss: 1.3453e-04\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 5s 52ms/step - loss: 1.3407e-04\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 5s 53ms/step - loss: 1.3329e-04\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 5s 52ms/step - loss: 1.3290e-04\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 5s 52ms/step - loss: 1.3248e-04\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 5s 52ms/step - loss: 1.3237e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "95\n",
      "96\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "99/99 [==============================] - 8s 45ms/step - loss: 1.4116e-04\n",
      "Epoch 2/10\n",
      "99/99 [==============================] - 5s 46ms/step - loss: 1.3683e-04\n",
      "Epoch 3/10\n",
      "99/99 [==============================] - 4s 45ms/step - loss: 1.3578e-04\n",
      "Epoch 4/10\n",
      "99/99 [==============================] - 4s 45ms/step - loss: 1.3542e-04\n",
      "Epoch 5/10\n",
      "99/99 [==============================] - 5s 45ms/step - loss: 1.3480e-04\n",
      "Epoch 6/10\n",
      "99/99 [==============================] - 4s 45ms/step - loss: 1.3448e-04\n",
      "Epoch 7/10\n",
      "99/99 [==============================] - 4s 45ms/step - loss: 1.3404e-04\n",
      "Epoch 8/10\n",
      "99/99 [==============================] - 5s 46ms/step - loss: 1.3390e-04\n",
      "Epoch 9/10\n",
      "99/99 [==============================] - 4s 45ms/step - loss: 1.3260e-04\n",
      "Epoch 10/10\n",
      "99/99 [==============================] - 5s 49ms/step - loss: 1.3277e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "96\n",
      "97\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 8s 45ms/step - loss: 1.4112e-04\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 4s 45ms/step - loss: 1.3743e-04\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 5s 47ms/step - loss: 1.3521e-04\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 5s 50ms/step - loss: 1.3545e-04\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 5s 48ms/step - loss: 1.3483e-04\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 5s 49ms/step - loss: 1.3325e-04\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 5s 49ms/step - loss: 1.3329e-04\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 5s 49ms/step - loss: 1.3252e-04\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 5s 49ms/step - loss: 1.3213e-04\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 5s 49ms/step - loss: 1.3184e-04\n",
      "2/2 [==============================] - 1s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "97\n",
      "98\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "101/101 [==============================] - 9s 55ms/step - loss: 1.3918e-04\n",
      "Epoch 2/10\n",
      "101/101 [==============================] - 5s 54ms/step - loss: 1.3684e-04\n",
      "Epoch 3/10\n",
      "101/101 [==============================] - 5s 54ms/step - loss: 1.3503e-04\n",
      "Epoch 4/10\n",
      "101/101 [==============================] - 5s 54ms/step - loss: 1.3491e-04\n",
      "Epoch 5/10\n",
      "101/101 [==============================] - 6s 55ms/step - loss: 1.3331e-04\n",
      "Epoch 6/10\n",
      "101/101 [==============================] - 6s 56ms/step - loss: 1.3252e-04\n",
      "Epoch 7/10\n",
      "101/101 [==============================] - 6s 56ms/step - loss: 1.3207e-04\n",
      "Epoch 8/10\n",
      "101/101 [==============================] - 6s 56ms/step - loss: 1.3160e-04\n",
      "Epoch 9/10\n",
      "101/101 [==============================] - 6s 56ms/step - loss: 1.3087e-04\n",
      "Epoch 10/10\n",
      "101/101 [==============================] - 6s 57ms/step - loss: 1.3083e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "98\n",
      "99\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "101/101 [==============================] - 10s 58ms/step - loss: 1.4376e-04\n",
      "Epoch 2/10\n",
      "101/101 [==============================] - 6s 58ms/step - loss: 1.3478e-04\n",
      "Epoch 3/10\n",
      "101/101 [==============================] - 6s 58ms/step - loss: 1.3421e-04\n",
      "Epoch 4/10\n",
      "101/101 [==============================] - 6s 58ms/step - loss: 1.3351e-04\n",
      "Epoch 5/10\n",
      "101/101 [==============================] - 6s 58ms/step - loss: 1.3320e-04\n",
      "Epoch 6/10\n",
      "101/101 [==============================] - 6s 58ms/step - loss: 1.3272e-04\n",
      "Epoch 7/10\n",
      "101/101 [==============================] - 6s 58ms/step - loss: 1.3243e-04\n",
      "Epoch 8/10\n",
      "101/101 [==============================] - 6s 58ms/step - loss: 1.3161e-04\n",
      "Epoch 9/10\n",
      "101/101 [==============================] - 6s 58ms/step - loss: 1.3080e-04\n",
      "Epoch 10/10\n",
      "101/101 [==============================] - 6s 58ms/step - loss: 1.3068e-04\n",
      "2/2 [==============================] - 1s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "99\n",
      "100\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "102/102 [==============================] - 10s 56ms/step - loss: 1.4078e-04\n",
      "Epoch 2/10\n",
      "102/102 [==============================] - 6s 56ms/step - loss: 1.3554e-04\n",
      "Epoch 3/10\n",
      "102/102 [==============================] - 6s 56ms/step - loss: 1.3498e-04\n",
      "Epoch 4/10\n",
      "102/102 [==============================] - 6s 55ms/step - loss: 1.3517e-04\n",
      "Epoch 5/10\n",
      "102/102 [==============================] - 6s 56ms/step - loss: 1.3409e-04\n",
      "Epoch 6/10\n",
      "102/102 [==============================] - 6s 57ms/step - loss: 1.3329e-04\n",
      "Epoch 7/10\n",
      "102/102 [==============================] - 6s 57ms/step - loss: 1.3261e-04\n",
      "Epoch 8/10\n",
      "102/102 [==============================] - 6s 57ms/step - loss: 1.3222e-04\n",
      "Epoch 9/10\n",
      "102/102 [==============================] - 6s 57ms/step - loss: 1.3200e-04\n",
      "Epoch 10/10\n",
      "102/102 [==============================] - 6s 57ms/step - loss: 1.3120e-04\n",
      "2/2 [==============================] - 1s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "100\n",
      "101\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "103/103 [==============================] - 10s 58ms/step - loss: 1.4035e-04\n",
      "Epoch 2/10\n",
      "103/103 [==============================] - 6s 59ms/step - loss: 1.3557e-04\n",
      "Epoch 3/10\n",
      "103/103 [==============================] - 6s 58ms/step - loss: 1.3464e-04\n",
      "Epoch 4/10\n",
      "103/103 [==============================] - 6s 58ms/step - loss: 1.3405e-04\n",
      "Epoch 5/10\n",
      "103/103 [==============================] - 6s 58ms/step - loss: 1.3364e-04\n",
      "Epoch 6/10\n",
      "103/103 [==============================] - 6s 59ms/step - loss: 1.3315e-04\n",
      "Epoch 7/10\n",
      "103/103 [==============================] - 6s 58ms/step - loss: 1.3253e-04\n",
      "Epoch 8/10\n",
      "103/103 [==============================] - 6s 58ms/step - loss: 1.3133e-04\n",
      "Epoch 9/10\n",
      "103/103 [==============================] - 6s 58ms/step - loss: 1.3120e-04\n",
      "Epoch 10/10\n",
      "103/103 [==============================] - 6s 58ms/step - loss: 1.3042e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 35ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "101\n",
      "102\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "104/104 [==============================] - 10s 56ms/step - loss: 1.3991e-04\n",
      "Epoch 2/10\n",
      "104/104 [==============================] - 6s 56ms/step - loss: 1.3427e-04\n",
      "Epoch 3/10\n",
      "104/104 [==============================] - 6s 56ms/step - loss: 1.3369e-04\n",
      "Epoch 4/10\n",
      "104/104 [==============================] - 6s 56ms/step - loss: 1.3293e-04\n",
      "Epoch 5/10\n",
      "104/104 [==============================] - 6s 56ms/step - loss: 1.3260e-04\n",
      "Epoch 6/10\n",
      "104/104 [==============================] - 6s 57ms/step - loss: 1.3087e-04\n",
      "Epoch 7/10\n",
      "104/104 [==============================] - 6s 56ms/step - loss: 1.3196e-04\n",
      "Epoch 8/10\n",
      "104/104 [==============================] - 6s 56ms/step - loss: 1.3050e-04\n",
      "Epoch 9/10\n",
      "104/104 [==============================] - 6s 56ms/step - loss: 1.2996e-04\n",
      "Epoch 10/10\n",
      "104/104 [==============================] - 6s 56ms/step - loss: 1.3002e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "102\n",
      "103\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "105/105 [==============================] - 10s 58ms/step - loss: 1.4010e-04\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 1.3773e-04\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 1.3599e-04\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 1.3393e-04\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 1.3195e-04\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 1.3155e-04\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 6s 59ms/step - loss: 1.3092e-04\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 1.3050e-04\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 6s 58ms/step - loss: 1.3022e-04\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 6s 60ms/step - loss: 1.2998e-04\n",
      "2/2 [==============================] - 1s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 27ms/step\n",
      "103\n",
      "104\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "105/105 [==============================] - 10s 56ms/step - loss: 1.3854e-04\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 6s 56ms/step - loss: 1.3473e-04\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 6s 56ms/step - loss: 1.3392e-04\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 6s 57ms/step - loss: 1.3216e-04\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 6s 56ms/step - loss: 1.3228e-04\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 6s 56ms/step - loss: 1.3138e-04\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 6s 56ms/step - loss: 1.3073e-04\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 6s 57ms/step - loss: 1.2983e-04\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 6s 56ms/step - loss: 1.2985e-04\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 6s 56ms/step - loss: 1.2921e-04\n",
      "2/2 [==============================] - 1s 7ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "104\n",
      "105\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "106/106 [==============================] - 10s 56ms/step - loss: 1.3719e-04\n",
      "Epoch 2/10\n",
      "106/106 [==============================] - 6s 56ms/step - loss: 1.3375e-04\n",
      "Epoch 3/10\n",
      "106/106 [==============================] - 6s 57ms/step - loss: 1.3245e-04\n",
      "Epoch 4/10\n",
      "106/106 [==============================] - 6s 56ms/step - loss: 1.3177e-04\n",
      "Epoch 5/10\n",
      "106/106 [==============================] - 6s 57ms/step - loss: 1.3084e-04\n",
      "Epoch 6/10\n",
      "106/106 [==============================] - 6s 57ms/step - loss: 1.3106e-04\n",
      "Epoch 7/10\n",
      "106/106 [==============================] - 6s 57ms/step - loss: 1.3036e-04\n",
      "Epoch 8/10\n",
      "106/106 [==============================] - 6s 57ms/step - loss: 1.2969e-04\n",
      "Epoch 9/10\n",
      "106/106 [==============================] - 6s 56ms/step - loss: 1.2887e-04\n",
      "Epoch 10/10\n",
      "106/106 [==============================] - 6s 57ms/step - loss: 1.2837e-04\n",
      "2/2 [==============================] - 1s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "105\n",
      "106\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "107/107 [==============================] - 9s 51ms/step - loss: 1.4821e-04\n",
      "Epoch 2/10\n",
      "107/107 [==============================] - 5s 51ms/step - loss: 1.4856e-04\n",
      "Epoch 3/10\n",
      "107/107 [==============================] - 5s 51ms/step - loss: 1.4638e-04\n",
      "Epoch 4/10\n",
      "107/107 [==============================] - 5s 51ms/step - loss: 1.4509e-04\n",
      "Epoch 5/10\n",
      "107/107 [==============================] - 5s 51ms/step - loss: 1.4481e-04\n",
      "Epoch 6/10\n",
      "107/107 [==============================] - 5s 50ms/step - loss: 1.4405e-04\n",
      "Epoch 7/10\n",
      "107/107 [==============================] - 5s 51ms/step - loss: 1.4310e-04\n",
      "Epoch 8/10\n",
      "107/107 [==============================] - 5s 51ms/step - loss: 1.4279e-04\n",
      "Epoch 9/10\n",
      "107/107 [==============================] - 6s 52ms/step - loss: 1.4201e-04\n",
      "Epoch 10/10\n",
      "107/107 [==============================] - 5s 51ms/step - loss: 1.4125e-04\n",
      "2/2 [==============================] - 1s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "106\n",
      "107\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "108/108 [==============================] - 9s 50ms/step - loss: 1.5109e-04\n",
      "Epoch 2/10\n",
      "108/108 [==============================] - 5s 50ms/step - loss: 1.4834e-04\n",
      "Epoch 3/10\n",
      "108/108 [==============================] - 5s 50ms/step - loss: 1.4612e-04\n",
      "Epoch 4/10\n",
      "108/108 [==============================] - 5s 50ms/step - loss: 1.4595e-04\n",
      "Epoch 5/10\n",
      "108/108 [==============================] - 5s 50ms/step - loss: 1.4581e-04\n",
      "Epoch 6/10\n",
      "108/108 [==============================] - 5s 50ms/step - loss: 1.4438e-04\n",
      "Epoch 7/10\n",
      "108/108 [==============================] - 5s 50ms/step - loss: 1.4337e-04\n",
      "Epoch 8/10\n",
      "108/108 [==============================] - 5s 50ms/step - loss: 1.4330e-04\n",
      "Epoch 9/10\n",
      "108/108 [==============================] - 5s 50ms/step - loss: 1.4300e-04\n",
      "Epoch 10/10\n",
      "108/108 [==============================] - 5s 50ms/step - loss: 1.4192e-04\n",
      "2/2 [==============================] - 1s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "107\n",
      "108\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "108/108 [==============================] - 11s 58ms/step - loss: 1.4969e-04\n",
      "Epoch 2/10\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 1.4687e-04\n",
      "Epoch 3/10\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 1.4577e-04\n",
      "Epoch 4/10\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 1.4440e-04\n",
      "Epoch 5/10\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 1.4413e-04\n",
      "Epoch 6/10\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 1.4328e-04\n",
      "Epoch 7/10\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 1.4242e-04\n",
      "Epoch 8/10\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 1.4207e-04\n",
      "Epoch 9/10\n",
      "108/108 [==============================] - 6s 58ms/step - loss: 1.4150e-04\n",
      "Epoch 10/10\n",
      "108/108 [==============================] - 6s 57ms/step - loss: 1.4074e-04\n",
      "2/2 [==============================] - 1s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "108\n",
      "109\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "109/109 [==============================] - 11s 60ms/step - loss: 1.5062e-04\n",
      "Epoch 2/10\n",
      "109/109 [==============================] - 7s 60ms/step - loss: 1.4891e-04\n",
      "Epoch 3/10\n",
      "109/109 [==============================] - 7s 60ms/step - loss: 1.4462e-04\n",
      "Epoch 4/10\n",
      "109/109 [==============================] - 7s 60ms/step - loss: 1.4588e-04\n",
      "Epoch 5/10\n",
      "109/109 [==============================] - 7s 60ms/step - loss: 1.4520e-04\n",
      "Epoch 6/10\n",
      "109/109 [==============================] - 7s 60ms/step - loss: 1.4369e-04\n",
      "Epoch 7/10\n",
      "109/109 [==============================] - 7s 60ms/step - loss: 1.4340e-04\n",
      "Epoch 8/10\n",
      "109/109 [==============================] - 7s 60ms/step - loss: 1.4257e-04\n",
      "Epoch 9/10\n",
      "109/109 [==============================] - 7s 60ms/step - loss: 1.4177e-04\n",
      "Epoch 10/10\n",
      "109/109 [==============================] - 7s 60ms/step - loss: 1.4225e-04\n",
      "2/2 [==============================] - 1s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "109\n",
      "110\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "110/110 [==============================] - 10s 55ms/step - loss: 1.4837e-04\n",
      "Epoch 2/10\n",
      "110/110 [==============================] - 6s 55ms/step - loss: 1.4649e-04\n",
      "Epoch 3/10\n",
      "110/110 [==============================] - 6s 56ms/step - loss: 1.4588e-04\n",
      "Epoch 4/10\n",
      "110/110 [==============================] - 6s 56ms/step - loss: 1.4456e-04\n",
      "Epoch 5/10\n",
      "110/110 [==============================] - 6s 56ms/step - loss: 1.4445e-04\n",
      "Epoch 6/10\n",
      "110/110 [==============================] - 6s 56ms/step - loss: 1.4358e-04\n",
      "Epoch 7/10\n",
      "110/110 [==============================] - 6s 56ms/step - loss: 1.4300e-04\n",
      "Epoch 8/10\n",
      "110/110 [==============================] - 6s 56ms/step - loss: 1.4139e-04\n",
      "Epoch 9/10\n",
      "110/110 [==============================] - 6s 56ms/step - loss: 1.4167e-04\n",
      "Epoch 10/10\n",
      "110/110 [==============================] - 6s 55ms/step - loss: 1.4077e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "110\n",
      "111\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "111/111 [==============================] - 9s 47ms/step - loss: 1.4882e-04\n",
      "Epoch 2/10\n",
      "111/111 [==============================] - 5s 47ms/step - loss: 1.4613e-04\n",
      "Epoch 3/10\n",
      "111/111 [==============================] - 5s 47ms/step - loss: 1.4493e-04\n",
      "Epoch 4/10\n",
      "111/111 [==============================] - 5s 47ms/step - loss: 1.4526e-04\n",
      "Epoch 5/10\n",
      "111/111 [==============================] - 5s 47ms/step - loss: 1.4382e-04\n",
      "Epoch 6/10\n",
      "111/111 [==============================] - 5s 47ms/step - loss: 1.4275e-04\n",
      "Epoch 7/10\n",
      "111/111 [==============================] - 5s 47ms/step - loss: 1.4239e-04\n",
      "Epoch 8/10\n",
      "111/111 [==============================] - 5s 48ms/step - loss: 1.4200e-04\n",
      "Epoch 9/10\n",
      "111/111 [==============================] - 5s 48ms/step - loss: 1.4102e-04\n",
      "Epoch 10/10\n",
      "111/111 [==============================] - 5s 48ms/step - loss: 1.4039e-04\n",
      "2/2 [==============================] - 1s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "111\n",
      "112\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 9s 46ms/step - loss: 1.4958e-04\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 5s 49ms/step - loss: 1.4574e-04\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 5s 48ms/step - loss: 1.4511e-04\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 5s 48ms/step - loss: 1.4410e-04\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 5s 48ms/step - loss: 1.4369e-04\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 5s 48ms/step - loss: 1.4225e-04\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 5s 49ms/step - loss: 1.4157e-04\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 5s 48ms/step - loss: 1.4065e-04\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 5s 48ms/step - loss: 1.4043e-04\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 5s 48ms/step - loss: 1.4043e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "112\n",
      "113\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 11s 56ms/step - loss: 1.4960e-04\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 6s 57ms/step - loss: 1.4594e-04\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 6s 56ms/step - loss: 1.4508e-04\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 6s 56ms/step - loss: 1.4341e-04\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 6s 56ms/step - loss: 1.4333e-04\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 6s 56ms/step - loss: 1.4215e-04\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 6s 56ms/step - loss: 1.4095e-04\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 6s 56ms/step - loss: 1.4090e-04\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 6s 56ms/step - loss: 1.3995e-04\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 6s 55ms/step - loss: 1.3945e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "113\n",
      "114\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "113/113 [==============================] - 11s 59ms/step - loss: 1.5109e-04\n",
      "Epoch 2/10\n",
      "113/113 [==============================] - 7s 58ms/step - loss: 1.4297e-04\n",
      "Epoch 3/10\n",
      "113/113 [==============================] - 7s 58ms/step - loss: 1.4286e-04\n",
      "Epoch 4/10\n",
      "113/113 [==============================] - 7s 58ms/step - loss: 1.4237e-04\n",
      "Epoch 5/10\n",
      "113/113 [==============================] - 7s 59ms/step - loss: 1.4187e-04\n",
      "Epoch 6/10\n",
      "113/113 [==============================] - 7s 59ms/step - loss: 1.4156e-04\n",
      "Epoch 7/10\n",
      "113/113 [==============================] - 7s 58ms/step - loss: 1.3999e-04\n",
      "Epoch 8/10\n",
      "113/113 [==============================] - 6s 55ms/step - loss: 1.3929e-04\n",
      "Epoch 9/10\n",
      "113/113 [==============================] - 6s 54ms/step - loss: 1.3934e-04\n",
      "Epoch 10/10\n",
      "113/113 [==============================] - 6s 54ms/step - loss: 1.3919e-04\n",
      "2/2 [==============================] - 1s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "114\n",
      "115\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 10s 50ms/step - loss: 1.4784e-04\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 1.4397e-04\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 1.4383e-04\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 1.4179e-04\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 1.4294e-04\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 6s 49ms/step - loss: 1.4156e-04\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 1.4014e-04\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 6s 49ms/step - loss: 1.3964e-04\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 6s 51ms/step - loss: 1.3921e-04\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 1.3873e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "115\n",
      "116\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "115/115 [==============================] - 10s 51ms/step - loss: 1.4759e-04\n",
      "Epoch 2/10\n",
      "115/115 [==============================] - 6s 51ms/step - loss: 1.4453e-04\n",
      "Epoch 3/10\n",
      "115/115 [==============================] - 6s 51ms/step - loss: 1.4217e-04\n",
      "Epoch 4/10\n",
      "115/115 [==============================] - 6s 51ms/step - loss: 1.4243e-04\n",
      "Epoch 5/10\n",
      "115/115 [==============================] - 6s 51ms/step - loss: 1.4189e-04\n",
      "Epoch 6/10\n",
      "115/115 [==============================] - 6s 51ms/step - loss: 1.4123e-04\n",
      "Epoch 7/10\n",
      "115/115 [==============================] - 6s 51ms/step - loss: 1.4075e-04\n",
      "Epoch 8/10\n",
      "115/115 [==============================] - 6s 51ms/step - loss: 1.3958e-04\n",
      "Epoch 9/10\n",
      "115/115 [==============================] - 6s 52ms/step - loss: 1.3974e-04\n",
      "Epoch 10/10\n",
      "115/115 [==============================] - 6s 54ms/step - loss: 1.3895e-04\n",
      "2/2 [==============================] - 1s 11ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "116\n",
      "117\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "115/115 [==============================] - 10s 55ms/step - loss: 1.4857e-04\n",
      "Epoch 2/10\n",
      "115/115 [==============================] - 6s 56ms/step - loss: 1.4563e-04\n",
      "Epoch 3/10\n",
      "115/115 [==============================] - 6s 56ms/step - loss: 1.4464e-04\n",
      "Epoch 4/10\n",
      "115/115 [==============================] - 7s 58ms/step - loss: 1.4474e-04\n",
      "Epoch 5/10\n",
      "115/115 [==============================] - 7s 59ms/step - loss: 1.4346e-04\n",
      "Epoch 6/10\n",
      "115/115 [==============================] - 7s 58ms/step - loss: 1.4239e-04\n",
      "Epoch 7/10\n",
      "115/115 [==============================] - 7s 58ms/step - loss: 1.4162e-04\n",
      "Epoch 8/10\n",
      "115/115 [==============================] - 7s 58ms/step - loss: 1.4100e-04\n",
      "Epoch 9/10\n",
      "115/115 [==============================] - 7s 58ms/step - loss: 1.4082e-04\n",
      "Epoch 10/10\n",
      "115/115 [==============================] - 7s 57ms/step - loss: 1.4025e-04\n",
      "2/2 [==============================] - 1s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "117\n",
      "118\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "116/116 [==============================] - 10s 50ms/step - loss: 1.4998e-04\n",
      "Epoch 2/10\n",
      "116/116 [==============================] - 6s 50ms/step - loss: 1.4529e-04\n",
      "Epoch 3/10\n",
      "116/116 [==============================] - 6s 51ms/step - loss: 1.4509e-04\n",
      "Epoch 4/10\n",
      "116/116 [==============================] - 6s 50ms/step - loss: 1.4391e-04\n",
      "Epoch 5/10\n",
      "116/116 [==============================] - 6s 51ms/step - loss: 1.4393e-04\n",
      "Epoch 6/10\n",
      "116/116 [==============================] - 6s 50ms/step - loss: 1.4286e-04\n",
      "Epoch 7/10\n",
      "116/116 [==============================] - 6s 51ms/step - loss: 1.4174e-04\n",
      "Epoch 8/10\n",
      "116/116 [==============================] - 6s 51ms/step - loss: 1.4159e-04\n",
      "Epoch 9/10\n",
      "116/116 [==============================] - 6s 50ms/step - loss: 1.4076e-04\n",
      "Epoch 10/10\n",
      "116/116 [==============================] - 6s 50ms/step - loss: 1.4056e-04\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "118\n",
      "119\n",
      "{'num_epochs': 10, 'batch_size': 64, 'hidden_size': 100, 'num_layers': 1, 'drop': 0.3, 'learning_rate': 0.05, 'bilstm': True}\n",
      "40 lll 4\n",
      "Epoch 1/10\n",
      "117/117 [==============================] - 11s 53ms/step - loss: 1.5081e-04\n",
      "Epoch 2/10\n",
      "117/117 [==============================] - 6s 51ms/step - loss: 1.4726e-04\n",
      "Epoch 3/10\n",
      "117/117 [==============================] - 6s 53ms/step - loss: 1.4478e-04\n",
      "Epoch 4/10\n",
      "117/117 [==============================] - 6s 53ms/step - loss: 1.4556e-04\n",
      "Epoch 5/10\n",
      "117/117 [==============================] - 6s 52ms/step - loss: 1.4442e-04\n",
      "Epoch 6/10\n",
      "117/117 [==============================] - 6s 54ms/step - loss: 1.4328e-04\n",
      "Epoch 7/10\n",
      "117/117 [==============================] - 6s 55ms/step - loss: 1.4308e-04\n",
      "Epoch 8/10\n",
      "117/117 [==============================] - 6s 54ms/step - loss: 1.4196e-04\n",
      "Epoch 9/10\n",
      "117/117 [==============================] - 6s 54ms/step - loss: 1.4159e-04\n",
      "Epoch 10/10\n",
      "117/117 [==============================] - 6s 54ms/step - loss: 1.4169e-04\n",
      "2/2 [==============================] - 1s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "119\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective':'regression',\n",
    "    'num_leaves': 100,\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'metric': 'l1',\n",
    "    'num_iterations': 200\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#tf.keras.utils.disable_interactive_logging()\n",
    "tf.keras.utils.enable_interactive_logging()\n",
    "seed(0)\n",
    "\n",
    "#model = lgb.LGBMRegressor()\n",
    "num_layers = 1\n",
    "k = 120\n",
    "model_types = ['BiLSTM'] # LSTM\n",
    "for j in model_types:\n",
    "    bilstm = True if j == 'BiLSTM' else False\n",
    "    \n",
    "    for i in range(1, num_layers + 1):\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "        print(j, i)\n",
    "        model = nn_wrapper_tf(j)\n",
    "        #model = lgb.LGBMRegressor()\n",
    "        params2 = {\n",
    "        'num_epochs': 10,\n",
    "        'batch_size' : 64,\n",
    "        'hidden_size': 100,\n",
    "        'num_layers': i,\n",
    "        'drop': 0.3,\n",
    "        'learning_rate': 0.05,\n",
    "        'bilstm': bilstm\n",
    "        }\n",
    "        results = model_forecasts(model, params2, k)#, True, np.array([1, 1, 1, 1, 1]))\n",
    "\n",
    "        results[0].to_csv(f'new_results_v6_{j}_{i}_{k}.csv')\n",
    "        results[1].to_csv(f'new_results_metrics_v6_{j}_{i}_{k}.csv')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
