{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import xgboost as xgb\n",
    "\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import datetime as dt\n",
    "from torch.autograd import Variable \n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import LSTM , Bidirectional\n",
    "color_pal = sns.color_palette()\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed(id):\n",
    "    torch.manual_seed(id)\n",
    "    np.random.seed(id)\n",
    "    random.seed(id)\n",
    "    tf.random.set_seed(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>^GSPC_Open</th>\n",
       "      <th>^GSPC_Volume</th>\n",
       "      <th>^GSPC_SQZ_20_2.0_20_1.5</th>\n",
       "      <th>^GSPC_SQZ_ON</th>\n",
       "      <th>^GSPC_ABER_ATR_5_15</th>\n",
       "      <th>^GSPC_ADOSC_3_10</th>\n",
       "      <th>^GSPC_ADX_14</th>\n",
       "      <th>^GSPC_DMP_14</th>\n",
       "      <th>^GSPC_DMN_14</th>\n",
       "      <th>^GSPC_AROOND_14</th>\n",
       "      <th>...</th>\n",
       "      <th>DX-Y.NYB level 50</th>\n",
       "      <th>^GSPC_High</th>\n",
       "      <th>^GSPC_Low</th>\n",
       "      <th>^GSPC_Close</th>\n",
       "      <th>DX-Y.NYB_High</th>\n",
       "      <th>DX-Y.NYB_Low</th>\n",
       "      <th>DX-Y.NYB_Close</th>\n",
       "      <th>VIX_Close</th>\n",
       "      <th>IXIC_Close</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>438.670013</td>\n",
       "      <td>247200000</td>\n",
       "      <td>7.049998</td>\n",
       "      <td>False</td>\n",
       "      <td>2.902007</td>\n",
       "      <td>6.886895e+07</td>\n",
       "      <td>18.006061</td>\n",
       "      <td>28.075804</td>\n",
       "      <td>22.869827</td>\n",
       "      <td>7.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>438.929993</td>\n",
       "      <td>436.910004</td>\n",
       "      <td>438.779999</td>\n",
       "      <td>92.610001</td>\n",
       "      <td>91.080002</td>\n",
       "      <td>92.459999</td>\n",
       "      <td>12.42</td>\n",
       "      <td>696.340027</td>\n",
       "      <td>1993-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>438.779999</td>\n",
       "      <td>238570000</td>\n",
       "      <td>7.881663</td>\n",
       "      <td>False</td>\n",
       "      <td>2.957872</td>\n",
       "      <td>1.703827e+08</td>\n",
       "      <td>18.460351</td>\n",
       "      <td>34.209112</td>\n",
       "      <td>20.804444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>442.519989</td>\n",
       "      <td>438.779999</td>\n",
       "      <td>442.519989</td>\n",
       "      <td>93.730003</td>\n",
       "      <td>92.419998</td>\n",
       "      <td>93.559998</td>\n",
       "      <td>12.33</td>\n",
       "      <td>701.770020</td>\n",
       "      <td>1993-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>442.519989</td>\n",
       "      <td>271560000</td>\n",
       "      <td>7.234996</td>\n",
       "      <td>False</td>\n",
       "      <td>2.901346</td>\n",
       "      <td>2.566182e+08</td>\n",
       "      <td>18.970118</td>\n",
       "      <td>33.292546</td>\n",
       "      <td>19.722291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>442.869995</td>\n",
       "      <td>440.760010</td>\n",
       "      <td>442.549988</td>\n",
       "      <td>94.040001</td>\n",
       "      <td>93.199997</td>\n",
       "      <td>93.919998</td>\n",
       "      <td>12.25</td>\n",
       "      <td>705.119995</td>\n",
       "      <td>1993-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>442.559998</td>\n",
       "      <td>345410000</td>\n",
       "      <td>7.144994</td>\n",
       "      <td>False</td>\n",
       "      <td>3.027924</td>\n",
       "      <td>3.715875e+08</td>\n",
       "      <td>20.417289</td>\n",
       "      <td>40.078885</td>\n",
       "      <td>17.493099</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>447.350006</td>\n",
       "      <td>442.559998</td>\n",
       "      <td>447.200012</td>\n",
       "      <td>94.599998</td>\n",
       "      <td>93.599998</td>\n",
       "      <td>94.239998</td>\n",
       "      <td>12.12</td>\n",
       "      <td>708.669983</td>\n",
       "      <td>1993-02-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>447.200012</td>\n",
       "      <td>351140000</td>\n",
       "      <td>8.356664</td>\n",
       "      <td>False</td>\n",
       "      <td>3.003394</td>\n",
       "      <td>4.713655e+08</td>\n",
       "      <td>22.193214</td>\n",
       "      <td>43.509068</td>\n",
       "      <td>16.387681</td>\n",
       "      <td>28.571429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>449.859985</td>\n",
       "      <td>447.200012</td>\n",
       "      <td>449.559998</td>\n",
       "      <td>94.860001</td>\n",
       "      <td>94.040001</td>\n",
       "      <td>94.529999</td>\n",
       "      <td>12.29</td>\n",
       "      <td>708.849976</td>\n",
       "      <td>1993-02-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 246 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ^GSPC_Open  ^GSPC_Volume  ^GSPC_SQZ_20_2.0_20_1.5  ^GSPC_SQZ_ON  \\\n",
       "0  438.670013     247200000                 7.049998         False   \n",
       "1  438.779999     238570000                 7.881663         False   \n",
       "2  442.519989     271560000                 7.234996         False   \n",
       "3  442.559998     345410000                 7.144994         False   \n",
       "4  447.200012     351140000                 8.356664         False   \n",
       "\n",
       "   ^GSPC_ABER_ATR_5_15  ^GSPC_ADOSC_3_10  ^GSPC_ADX_14  ^GSPC_DMP_14  \\\n",
       "0             2.902007      6.886895e+07     18.006061     28.075804   \n",
       "1             2.957872      1.703827e+08     18.460351     34.209112   \n",
       "2             2.901346      2.566182e+08     18.970118     33.292546   \n",
       "3             3.027924      3.715875e+08     20.417289     40.078885   \n",
       "4             3.003394      4.713655e+08     22.193214     43.509068   \n",
       "\n",
       "   ^GSPC_DMN_14  ^GSPC_AROOND_14  ...  DX-Y.NYB level 50  ^GSPC_High  \\\n",
       "0     22.869827         7.142857  ...                0.0  438.929993   \n",
       "1     20.804444         0.000000  ...                0.0  442.519989   \n",
       "2     19.722291         0.000000  ...                0.0  442.869995   \n",
       "3     17.493099        35.714286  ...                0.0  447.350006   \n",
       "4     16.387681        28.571429  ...                0.0  449.859985   \n",
       "\n",
       "    ^GSPC_Low  ^GSPC_Close  DX-Y.NYB_High  DX-Y.NYB_Low  DX-Y.NYB_Close  \\\n",
       "0  436.910004   438.779999      92.610001     91.080002       92.459999   \n",
       "1  438.779999   442.519989      93.730003     92.419998       93.559998   \n",
       "2  440.760010   442.549988      94.040001     93.199997       93.919998   \n",
       "3  442.559998   447.200012      94.599998     93.599998       94.239998   \n",
       "4  447.200012   449.559998      94.860001     94.040001       94.529999   \n",
       "\n",
       "   VIX_Close  IXIC_Close        Date  \n",
       "0      12.42  696.340027  1993-01-29  \n",
       "1      12.33  701.770020  1993-02-01  \n",
       "2      12.25  705.119995  1993-02-02  \n",
       "3      12.12  708.669983  1993-02-03  \n",
       "4      12.29  708.849976  1993-02-04  \n",
       "\n",
       "[5 rows x 246 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('filtered_features.csv', index_col='Date')\n",
    "df['Date'] = df.index\n",
    "df.index = np.array(range(df.shape[0]))\n",
    "df = df.drop('Unnamed: 0', axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7534,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Date'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final k 120\n"
     ]
    }
   ],
   "source": [
    "def save_data(data, time_index_loc, name):\n",
    "    data = pd.DataFrame(data)            \n",
    "    data['Date'] = time_index_loc\n",
    "    data.to_csv(name)\n",
    "\n",
    "def series_data_split_v2(data , time_index,  lag, horizon, t_train, t_val, t_test, name, step = 500):\n",
    "    # data - в виде серии\n",
    "     \n",
    "    data = np.array(data.to_list()).reshape(-1, 1)\n",
    "    indexes = np.array(range(len(data)))\n",
    "    \n",
    "    k = 0\n",
    "\n",
    "    while t_val < indexes[-1]:\n",
    "        x_train = []\n",
    "        y_train = []\n",
    "        x_test = []\n",
    "        y_test = []\n",
    "        x_val = []\n",
    "        y_val = []\n",
    "\n",
    "        if k == 0:\n",
    "            #scaler = MinMaxScaler(feature_range=(0,1))\n",
    "            #scaled_train_data = scaler.fit_transform(data[:t_train]) \n",
    "            #scaled = scaler.transform(data[: t_val])\n",
    "\n",
    "            scaled = np.array(data[: t_val]).reshape(1, -1)[0]\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            for i in range(lag + 1, t_train + 1):\n",
    "                x_train.append(np.log(scaled[i-lag:i] / scaled[i-lag-1:i-1])) \n",
    "                y_train.append(np.log(scaled[i]/scaled[i-1]))\n",
    "            \n",
    "            for i in range(t_train + 1, t_val):\n",
    "                x_val.append(np.log(scaled[i-lag:i] / scaled[i-lag-1:i-1]))\n",
    "                if i + horizon <= t_val:\n",
    "                    y_val.append(np.log(scaled[i : i + horizon]/scaled[i-1:i+horizon-1]))\n",
    "                else:\n",
    "                    y_val.append(np.log(scaled[i : t_val]/scaled[i-1:t_val-1]))\n",
    "            \n",
    "            \n",
    "            save_data(x_train, time_index.iloc[indexes[lag : t_train]].to_list(), f'x_train_v2_{name}_{k}.csv')\n",
    "            save_data(y_train, time_index.iloc[indexes[lag : t_train]].to_list(), f'y_train_v2_{name}_{k}.csv')\n",
    "            \n",
    "            \n",
    "            save_data(x_val, time_index.iloc[indexes[t_train: t_val - 1]].to_list(), f'x_val_v2_{name}_{k}.csv')\n",
    "            save_data(y_val, time_index.iloc[indexes[t_train: t_val - 1]].to_list(), f'y_val_v2_{name}_{k}.csv')\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "        #scaler = MinMaxScaler(feature_range=(0,1))\n",
    "        #scaled_train_data = scaler.fit_transform(data[:t_val])\n",
    "        scaled_train_data = np.array(data[:t_val]).reshape(1, -1)[0]\n",
    "        #scaler_filename = f\"scaler{k + 1}.save\"\n",
    "        #joblib.dump(scaler, scaler_filename) \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        x_train = []\n",
    "        y_train = []\n",
    "        x_test = []\n",
    "        y_test = []\n",
    "        x_val = []\n",
    "        y_val = []\n",
    "\n",
    "        \n",
    "        for i in range(lag + 1, t_val):\n",
    "            x_train.append(np.log(scaled_train_data[i-lag:i]/scaled_train_data[i-lag-1:i-1]))\n",
    "            y_train.append(np.log(scaled_train_data[i]/scaled_train_data[i-1]))\n",
    "\n",
    "        save_data(x_train, time_index.iloc[indexes[lag : t_val - 1]].to_list(), f'x_train_v2_{name}_{k + 1}.csv')\n",
    "        save_data(y_train, time_index.iloc[indexes[lag : t_val - 1]].to_list(), f'y_train_v2_{name}_{k + 1}.csv')\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        scaled_test = np.array(data[:t_test])\n",
    "        scaled_test = scaled_test.reshape(1, -1)[0]\n",
    "\n",
    "        pointer = 0\n",
    "        for i in range(t_val, t_test):\n",
    "            \n",
    "            x_test.append(np.log(scaled_test[i-lag:i]/scaled_test[i-lag-1:i-1])) \n",
    "            if i + horizon <= t_test:\n",
    "                y_test.append(np.log(scaled_test[i : i + horizon]/scaled_test[i-1:i+horizon-1]))\n",
    "            else:\n",
    "                y_test.append(np.log(scaled_test[i : t_test]/scaled_test[i-1 : t_test-1]))\n",
    "            \n",
    "              \n",
    "       \n",
    "        save_data(x_test, time_index.iloc[indexes[t_val  - 1: t_test - 1]].to_list(), f'x_test_v2_{name}_{k + 1}.csv')\n",
    "        save_data(y_test, time_index.iloc[indexes[t_val  - 1: t_test - 1]].to_list(), f'y_test_v2_{name}_{k + 1}.csv')\n",
    "          \n",
    "        \n",
    "        \n",
    "        #save_data(x_test[:-horizon], time_index.iloc[indexes[t_val - 1: t_test - horizon]].to_list(), f'x_val_{k + 1}.csv')\n",
    "        #save_data(y_test[:-horizon], time_index.iloc[indexes[t_val - 1: t_test - horizon]].to_list(), f'y_val_{k + 1}.csv')\n",
    "        \n",
    "        k += 1\n",
    "        t_train = t_val\n",
    "        \n",
    "\n",
    "        t_val = t_test \n",
    "        t_test += step\n",
    "        if t_test > indexes[-1]:\n",
    "            t_test = indexes[-1] + 1\n",
    "\n",
    "           \n",
    "        \n",
    "    print('final k', k)\n",
    "name = 'IXIC_Close' #'DX-Y.NYB_Close' 'VIX_Close       \n",
    "ser = df[name]\n",
    "#filtered['VIX_Close'] = data['^VIX_Close']\n",
    "#filtered['IXIC_Close']\n",
    "time_index = df['Date']\n",
    "lag = 40\n",
    "horizon = 10\n",
    "t_train = 1500\n",
    "t_val = 1550\n",
    "t_test = 1600\n",
    "step = 50\n",
    "\n",
    "series_data_split_v2(ser , time_index, lag, horizon, t_train, t_val, t_test, name, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM, GRU, Bi-LSTM, Sequencies\n",
    "\n",
    "\n",
    "class LSTM_Prediction_Model(nn.Module):\n",
    "    def __init__(self,  input_size, hidden_size, num_layers, output_size, drop, bilstm=False):\n",
    "        super(LSTM_Prediction_Model, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        \n",
    "       \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True, dropout=drop, bidirectional=bilstm) #lstm\n",
    "        self.fc =  nn.Linear(hidden_size, output_size) #fully connected 1\n",
    "        \n",
    "       \n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_() #hidden state\n",
    "        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_() #internal state\n",
    "       \n",
    "        # Propagate input through LSTM\n",
    "        print(x)\n",
    "        output, (hn, cn) = self.lstm(x, (h_0.detach(), c_0.detach())) #lstm with input, hidden, and internal state\n",
    "        output = self.fc(output[:, -1, :]) #reshaping the data for Dense layer next\n",
    "        \n",
    "        \n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_wrapper_torch:\n",
    "    def __init__(self, nn_type):\n",
    "        self.type = nn_type\n",
    "        self.num_epochs = 100\n",
    "        self.trial = 0\n",
    "        self.epochs = []\n",
    "    def set_params(self, **params):\n",
    "        \n",
    "        \n",
    "        self.num_epochs = params['num_epochs']\n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.batch_size = params['batch_size']\n",
    "        self.hidden_size = params['hidden_size'] #, num_layers, seq_length, drop\n",
    "        self.num_layers = params['num_layers']\n",
    "        self.drop = params['drop']\n",
    "        self.patience = params['patience']\n",
    "        if 'bilstm' in params.keys():\n",
    "            self.bilstm = True\n",
    "        else:\n",
    "            self.bilstm = False\n",
    "\n",
    "    def fit(self, X, y, X_val = 0, y_val = 0):\n",
    "\n",
    "        if isinstance(X, np.ndarray):\n",
    "            self.X_train = torch.from_numpy(np.reshape(X, (X.shape[0], X.shape[1], 1))).to(device = config.device, dtype = torch.float)\n",
    "            self.y_train = torch.from_numpy(np.array(y).reshape((len(y), 1))).to(device = config.device, dtype = torch.float)\n",
    "        else:\n",
    "            self.X_train = torch.from_numpy(np.reshape(X.values, (X.shape[0], X.shape[1], 1))).to(device = config.device, dtype = torch.float)\n",
    "            self.y_train = torch.from_numpy(np.array(y.values).reshape((len(y), 1))).to(device = config.device, dtype = torch.float)\n",
    "        \n",
    "\n",
    "        \n",
    "        if isinstance(X_val, np.ndarray):\n",
    "            self.X_val = torch.from_numpy(np.reshape(X_val, (X_val.shape[0], X_val.shape[1], 1))).to(device = config.device, dtype = torch.float)\n",
    "            self.y_val = torch.from_numpy(np.array(y_val[:, 0]).reshape((len(y_val[:, 0]), 1))).to(device = config.device, dtype = torch.float)\n",
    "\n",
    "        early_stopper = EarlyStopper(patience=self.patience, min_delta=0.1)\n",
    "        self.model = LSTM_Prediction_Model(X.shape[1], self.hidden_size, self.num_layers, 1, self.drop, self.bilstm)\n",
    "        criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate) \n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.9)\n",
    "        data_loss = []\n",
    "        data_val_loss = []\n",
    "\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            outputs = self.model(self.X_train) #forward pass\n",
    "            optimizer.zero_grad() #calculate the gradient, manually setting to 0\n",
    "            \n",
    "            # obtain the loss function\n",
    "            loss = criterion(outputs, self.y_train)\n",
    "            \n",
    "            loss.backward() #calculates the loss of the loss function\n",
    "            \n",
    "            optimizer.step() #improve from loss, i.e backprop\n",
    "            scheduler.step()\n",
    "            if isinstance(X_val, np.ndarray):\n",
    "                with torch.no_grad():\n",
    "                    self.model.eval()\n",
    "                    validation_loss = criterion(self.model(self.X_val), self.y_val)\n",
    "                    print(epoch, loss.item(), validation_loss.item())\n",
    "                    data_loss.append(loss.item())\n",
    "                    data_val_loss.append(validation_loss.item())\n",
    "                if early_stopper.early_stop(validation_loss) and epoch > 40:  \n",
    "                    print(epoch)\n",
    "                          \n",
    "                    break\n",
    "        return data_loss, data_val_loss            \n",
    "    def predict(self, x):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            return self.model(torch.from_numpy(np.reshape(x, (1, x.shape[1], 1))).to(device = config.device, dtype = torch.float)).to(device = 'cpu').detach().numpy()[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_predictor:\n",
    "    def __init__(self, input_shape, seq_num, inner_output, output_shape, num_layers, drop, bilstm = False):\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        ret_seq = True if num_layers > 1 else False\n",
    "        if not bilstm:\n",
    "            self.model.add(LSTM(units = inner_output, return_sequences = ret_seq, input_shape = (input_shape, seq_num)))\n",
    "            self.model.add(Dropout(drop))\n",
    "            if num_layers > 1:\n",
    "                if num_layers > 2:\n",
    "                    for i in range(num_layers-2):\n",
    "                        self.model.add(LSTM(units = inner_output, return_sequences = True))\n",
    "                        self.model.add(Dropout(drop))\n",
    "\n",
    "                self.model.add(LSTM(units = inner_output))\n",
    "                self.model.add(Dropout(drop))\n",
    "            self.model.add(Dense(units = output_shape))\n",
    "        else:\n",
    "        \n",
    "            # First layer of BiLSTM\n",
    "            self.model.add(Bidirectional(LSTM(units = inner_output, return_sequences = ret_seq, input_shape = (input_shape, seq_num))))\n",
    "            self.model.add(Dropout(drop))\n",
    "            if num_layers > 1:\n",
    "                if num_layers > 2:\n",
    "                    for i in range(num_layers-2):\n",
    "                        self.model.add(Bidirectional(LSTM(units = inner_output, return_sequences = True)))\n",
    "                        self.model.add(Dropout(drop))\n",
    "\n",
    "            \n",
    "                # Second layer of BiLSTM\n",
    "                self.model.add(Bidirectional(LSTM(units = inner_output)))\n",
    "                self.model.add(Dropout(drop))\n",
    "            self.model.add(Dense(units = output_shape))\n",
    "            \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_wrapper_tf:\n",
    "    def __init__(self, nn_type):\n",
    "        self.type = nn_type\n",
    "        \n",
    "        \n",
    "    def set_params(self, **params):\n",
    "    \n",
    "        self.num_epochs = params['num_epochs']\n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.batch_size = params['batch_size']\n",
    "        self.hidden_size = params['hidden_size'] #, num_layers, seq_length, drop\n",
    "        self.num_layers = params['num_layers']\n",
    "        self.drop = params['drop']\n",
    "        if 'bilstm' in params.keys():\n",
    "\n",
    "            self.bilstm = params['bilstm']\n",
    "        else:\n",
    "            self.bilstm = False\n",
    "        \n",
    "\n",
    "    def fit(self, X, y, X_val = 0, y_val = 0):\n",
    "        \n",
    "        \n",
    "        if isinstance(X, np.ndarray):\n",
    "            self.X_train = np.reshape(X, (X.shape[1], X.shape[2], X.shape[0]))\n",
    "            self.y_train = np.array(y).reshape((len(y), 1))\n",
    "        else:\n",
    "            self.X_train = np.reshape(X.values, (X.shape[1], X.shape[2], X.shape[0]))\n",
    "            self.y_train = np.array(y.values).reshape((len(y), 1))\n",
    "        print(X.shape[2],'lll', X.shape[0])\n",
    "        # (self, input_shape, inner_output, output_shape, num_layers, drop)\n",
    "        self.nn = LSTM_predictor(X.shape[2], X.shape[0], self.hidden_size, 1, self.num_layers, self.drop, self.bilstm)\n",
    "        self.nn.model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "        self.nn.model.fit(self.X_train, self.y_train, epochs = self.num_epochs, batch_size = self.batch_size, shuffle = False)\n",
    "                \n",
    "                   \n",
    "    def predict(self, x):\n",
    "        \n",
    "        return self.nn.model.predict(np.reshape(x, (x.shape[1], x.shape[2], x.shape[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# тестирую на бустинге\n",
    "import optuna\n",
    "\n",
    "\n",
    "def optuna_hyper_opt(model, X_train, y_train, X_val, y_val, params, metric_weights, num_lags):\n",
    "    \n",
    "    \n",
    "    def objective(trial):\n",
    "        parameters = {}\n",
    "        for i in params:\n",
    "            \n",
    "            if type(params[i]) == list:\n",
    "                if type(params[i][0]) == int:\n",
    "                    parameters.update({i: trial.suggest_int(i, params[i][0], params[i][1])})\n",
    "                elif type(params[i][0]) == float:\n",
    "                    parameters.update({i: trial.suggest_float(i, params[i][0], params[i][1])})\n",
    "                elif type(params[i][0]) == str:\n",
    "                    parameters.update({i: trial.suggest_categorical(i, params[i])})\n",
    "            else:\n",
    "                parameters.update({i:params[i]})\n",
    "        \n",
    "        print(parameters)\n",
    "        \n",
    "        model.set_params(**parameters)\n",
    "\n",
    "        if isinstance(model, nn_wrapper_tf):\n",
    "            model.fit(X_train, y_train, X_val, y_val)\n",
    "        else:\n",
    "            print('k o')\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            '''\n",
    "        pred = []\n",
    "        for i in X_val:\n",
    "            pred.append(horizon_forecasts(model, i, y_val.shape[1], num_lags))\n",
    "         '''\n",
    "        \n",
    "        pred = horizon_forecasts_v2(model, X_val, y_val.shape[1], num_lags)\n",
    "        score = np.array(scores_fun(pred, y_val))\n",
    "\n",
    "        return np.dot(score, metric_weights)\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=3, timeout=300)\n",
    "    return study.best_params\n",
    "\n",
    "def horizon_forecasts_v2(model, x, horizon, num_lags):\n",
    "    \n",
    "    all_features = x\n",
    "    if isinstance(model, nn_wrapper_tf):\n",
    "        for i in range(horizon):\n",
    "            f = model.predict(all_features).reshape(-1, 1)\n",
    "            \n",
    "            if i == 0:\n",
    "                prediction = f\n",
    "            else: \n",
    "                prediction = np.concatenate((prediction, f), axis=1)\n",
    "            lags = all_features[0]\n",
    "            \n",
    "            not_lags = all_features[1:]\n",
    "            \n",
    "            lags = np.concatenate((lags[:, 1:], f), axis=1)\n",
    "            #all_features = np.array([lags, not_lags])\n",
    "            \n",
    "            \n",
    "            all_features = np.concatenate([np.array([lags]), not_lags])\n",
    "    elif num_lags < x.shape[1]:\n",
    "        not_lags = all_features[:, num_lags:]\n",
    "        lags = all_features[:, :num_lags]\n",
    "        for i in range(horizon):\n",
    "            f = model.predict(all_features).reshape(-1, 1)\n",
    "            \n",
    "            if i == 0:\n",
    "                prediction = f\n",
    "            else: \n",
    "                prediction = np.concatenate((prediction, f), axis=1)\n",
    "            \n",
    "            lags = np.concatenate((lags[:, 1:], f), axis=1)\n",
    "            all_features = np.concatenate((lags, not_lags), axis=1)\n",
    "    else:\n",
    "        lags = all_features\n",
    "        for i in range(horizon):\n",
    "            f = model.predict(lags).reshape(-1, 1)\n",
    "            \n",
    "            if i == 0:\n",
    "                prediction = f\n",
    "            else: \n",
    "                prediction = np.concatenate((prediction, f), axis=1)\n",
    "\n",
    "            lags =  np.concatenate((lags[:, 1:], f), axis=1)\n",
    "\n",
    "    return np.array(prediction)\n",
    "\n",
    "def horizon_forecasts(model, x, horizon, num_lags):\n",
    "    prediction = []\n",
    "    all_features = x.reshape(1, -1)\n",
    "    if num_lags < len(x):\n",
    "        not_lags = all_features[0][num_lags:]\n",
    "        lags = all_features[0][:num_lags]\n",
    "        for i in range(horizon):\n",
    "            f = model.predict(all_features)[0]\n",
    "            prediction.append(f)\n",
    "            lags = np.append(lags[1:], f)\n",
    "            all_features = np.append(lags, not_lags).reshape(1, -1)\n",
    "            \n",
    "    else:\n",
    "        lags = all_features\n",
    "        for i in range(horizon):\n",
    "            f = model.predict(lags)[0]\n",
    "            prediction.append(f)\n",
    "            lags = np.append(lags[0][1:], f).reshape(1, -1)\n",
    "       \n",
    "       \n",
    "\n",
    "    return np.array(prediction)\n",
    "\n",
    "def scores_fun(pred, y):\n",
    "    horizon = y.shape[1]\n",
    "    n = y.shape[0]\n",
    "    metric_mse = 0\n",
    "    metric_trend_detect = 0\n",
    "    metric_weights = 0\n",
    "    metric_mape = 0\n",
    "    metric_true_pred = 0\n",
    "    \n",
    "    weights = np.array([i / horizon for i in range(1, horizon + 1)])\n",
    "\n",
    "    for i in range(n - horizon + 1):\n",
    "        diff = (pred[i] - y[i]) ** 2\n",
    "        if pred[i][0] * y[i][0] > 0:\n",
    "             metric_true_pred += 1\n",
    "        metric_mape += mean_absolute_percentage_error(y[i], pred[i])\n",
    "\n",
    "        metric_mse += np.sum(diff) / horizon\n",
    "        \n",
    "        if np.dot(np.sum(pred[i]), np.sum(y[i])) > 0:\n",
    "            metric_trend_detect += 1\n",
    "        \n",
    "        metric_weights += np.sum(np.dot(weights, diff)) / horizon\n",
    "\n",
    "    for ind, j in enumerate(range(n - horizon + 1, n)):\n",
    "        s1 = 0\n",
    "        s1_w = 0\n",
    "        s_mape = 0\n",
    "        s_tr_det = 0\n",
    "        s_tr_det_pred = 0\n",
    "        for i in range(horizon - ind - 1):\n",
    "            diff = (pred[j][i] - y[j][i]) ** 2 \n",
    "            s1 += diff\n",
    "            if i == 0:\n",
    "                if pred[j][i] * y[j][i] > 0:\n",
    "                    metric_true_pred += 1\n",
    "            if y[j][i] != 0:\n",
    "                s_mape += np.abs(pred[j][i] - y[j][i]) / y[j][i]\n",
    "            s1_w = diff * weights[i]\n",
    "            s_tr_det += y[j][i]\n",
    "            s_tr_det_pred += pred[j][i]\n",
    "            if i == horizon - ind - 1 - 1:\n",
    "                if np.dot(s_tr_det_pred, s_tr_det ) > 0:\n",
    "                    metric_trend_detect += 1 - ind / horizon\n",
    "        \n",
    "        metric_mse += s1 / (horizon - ind - 1)\n",
    "        metric_weights += s1_w / (horizon - ind -1)\n",
    "        metric_mape += s_mape / (horizon - ind - 1)\n",
    "\n",
    "\n",
    "    return [metric_mse/n, metric_weights/n, metric_trend_detect/n, 100 * metric_mape / n, metric_true_pred / n]\n",
    "\n",
    "\n",
    "def horizon_prediction(model, X, y, k, num_lags, addit_data=0):\n",
    "    time_index = X['Date']\n",
    "    if isinstance(model, nn_wrapper_tf):\n",
    "        X = X.drop('Date', axis = 1).values\n",
    "        X = np.concatenate([np.array([X]), np.array(addit_data)])\n",
    "        y = y.drop('Date', axis = 1).values\n",
    "    else:\n",
    "        X = X.drop('Date', axis = 1).values\n",
    "        y = y.drop('Date', axis = 1).values\n",
    "\n",
    "\n",
    "    hor = y.shape[1]\n",
    "    pred = []\n",
    "    '''\n",
    "    for i in X:\n",
    "        pred.append(horizon_forecasts_v2(model, i, hor, num_lags))\n",
    "    '''\n",
    "    \n",
    "    pred = horizon_forecasts_v2(model, X, hor, num_lags)\n",
    "    # pred в df превратить\n",
    "    score = scores_fun(pred, y)\n",
    "    \n",
    "    score.append(time_index[0])\n",
    "    score.append(time_index[time_index.shape[0] - 1])\n",
    "    #scaler = joblib.load(f\"scaler{k}.save\")\n",
    "\n",
    "    #pred = pd.DataFrame(list(map(lambda x: scaler.inverse_transform(x.reshape(-1, 1)).reshape(1, -1)[0], pred)), index=time_index)\n",
    "    pred = pd.DataFrame(pred, index=time_index)\n",
    "    return pred, score\n",
    "\n",
    "\n",
    "def model_forecasts(model, params_set, k, HPO=False, metric_weights = np.array([0, 0, 0, 0, 0])):\n",
    "    forecasts = []\n",
    "    metrics = []\n",
    "    df = pd.read_csv('filtered_features.csv', index_col='Date')\n",
    "    df['Date'] = df.index\n",
    "    df.index = np.array(range(df.shape[0]))\n",
    "    df = df.drop(['Unnamed: 0'], axis = 1)\n",
    "    tick = ['IXIC_Close', 'DX-Y.NYB_Close', 'VIX_Close']\n",
    "    for i in range(k):\n",
    "\n",
    "        print(i)\n",
    "        if i == 0:\n",
    "            data_x_train_val = pd.read_csv(f'x_train_v2_{i}.csv').drop(['Unnamed: 0', 'Date'], axis = 1)\n",
    "            num_lags = data_x_train_val.shape[1] - 1\n",
    "            if isinstance(model, nn_wrapper_tf):\n",
    "                \n",
    "                addit_data = []\n",
    "                for t in tick:\n",
    "                    add_data = pd.read_csv(f'x_train_v2_{t}_{i}.csv', index_col='Date')\n",
    "                    add_data['Date'] = add_data.index\n",
    "                    add_data.index = np.array(range(add_data.shape[0]))\n",
    "                    add_data = add_data.drop(['Unnamed: 0', 'Date'], axis = 1)\n",
    "                    #data_x_train_val = pd.merge(data_x_train_val, add_data, 'inner', 'Date').drop('Date', axis=1)\n",
    "                    addit_data.append(add_data.values)\n",
    "                \n",
    "                data_x_train_val = np.concatenate([np.array([data_x_train_val.values]), np.array(addit_data)])\n",
    "            else:\n",
    "                data_x_train_val = pd.merge(data_x_train_val, df, 'inner', 'Date').drop('Date', axis=1)\n",
    "\n",
    "            data_y_train_val = pd.read_csv(f'./y_train_v2_/y_train_v2_{i}.csv').drop(['Unnamed: 0', 'Date'], axis = 1)\n",
    "            data_x_val = pd.read_csv(f'./x_val_v2_{i}.csv').drop(['Unnamed: 0'], axis = 1)\n",
    "            if isinstance(model, nn_wrapper_tf):\n",
    "                \n",
    "                addit_data = []\n",
    "                for t in tick:\n",
    "                    add_data = pd.read_csv(f'./x_val_v2_{t}_{i}.csv', index_col='Date')\n",
    "                    add_data['Date'] = add_data.index\n",
    "                    add_data.index = np.array(range(add_data.shape[0]))\n",
    "                    add_data = add_data.drop(['Unnamed: 0', 'Date'], axis = 1)\n",
    "                    addit_data.append(add_data.values)\n",
    "                data_x_val = np.array([data_x_val.values, addit_data])\n",
    "                \n",
    "            else:\n",
    "                data_x_val = pd.merge(data_x_val, df, 'inner', 'Date').drop('Date', axis=1)\n",
    "\n",
    "            data_y_val = pd.read_csv(f'y_val_v2_{i}.csv').drop(['Unnamed: 0', 'Date'], axis = 1)\n",
    "        else:\n",
    "            data_x_train_val = data_x_train_test\n",
    "            data_y_train_val = data_y_train_test\n",
    "            data_x_val = data_x_test.drop( 'Date', axis = 1)\n",
    "            data_y_val = data_y_test.drop('Date', axis = 1)\n",
    "        \n",
    "        if HPO:\n",
    "            params = optuna_hyper_opt(model, data_x_train_val.values, data_y_train_val.values, data_x_val.values, data_y_val.values, params_set, metric_weights, num_lags)\n",
    "            for j in params_set:\n",
    "                if j not in params.keys():\n",
    "                    params.update({j : params_set[j]})    \n",
    "        else:\n",
    "            params = params_set \n",
    "\n",
    "        print(params)\n",
    "        model.set_params(**params)\n",
    "        \n",
    "\n",
    "        data_x_train_test = pd.read_csv(f'./x_train_v2_/x_train_v2_{i + 1}.csv').drop(['Unnamed: 0'], axis = 1)\n",
    "        if isinstance(model, nn_wrapper_tf):\n",
    "            addit_data = []\n",
    "            for t in tick:\n",
    "                add_data = pd.read_csv(f'./x_train_v2_{t}_/x_train_v2_{t}_{i + 1}.csv', index_col='Date')\n",
    "                add_data['Date'] = add_data.index\n",
    "                add_data.index = np.array(range(add_data.shape[0]))\n",
    "                add_data = add_data.drop(['Unnamed: 0', 'Date'], axis = 1)\n",
    "                addit_data.append(add_data.values)\n",
    "            \n",
    "            data_x_train_test = np.concatenate([np.array([data_x_train_test.drop(['Date'], axis = 1).values]), np.array(addit_data)])\n",
    "        else:\n",
    "            data_x_train_test = pd.merge(data_x_train_test, df, 'inner', 'Date').drop('Date', axis=1)\n",
    "            \n",
    "        data_y_train_test = pd.read_csv(f'./y_train_v2_/y_train_v2_{i + 1}.csv').drop(['Unnamed: 0', 'Date'], axis = 1)\n",
    "        data_x_test = pd.read_csv(f'./x_test_v2_/x_test_v2_{i + 1}.csv').drop('Unnamed: 0', axis = 1)\n",
    "        if not isinstance(model, nn_wrapper_tf):\n",
    "            data_x_test = pd.merge(data_x_test, df, 'inner', 'Date')\n",
    "        else:\n",
    "            addit_data = []\n",
    "            for t in tick:\n",
    "                add_data = pd.read_csv(f'./x_test_v2_{t}_/x_test_v2_{t}_{i + 1}.csv', index_col='Date')\n",
    "                add_data['Date'] = add_data.index\n",
    "                add_data.index = np.array(range(add_data.shape[0]))\n",
    "                add_data = add_data.drop(['Unnamed: 0', 'Date'], axis = 1)\n",
    "                addit_data.append(add_data.values)\n",
    "\n",
    "        data_y_test = pd.read_csv(f'./y_test_v2_/y_test_v2_{i + 1}.csv').drop('Unnamed: 0', axis = 1)\n",
    "        #model = model(params)\n",
    "        model.fit(data_x_train_test, data_y_train_test)\n",
    "        res = horizon_prediction(model, data_x_test, data_y_test, i + 1, num_lags, addit_data)\n",
    "\n",
    "        \n",
    "        forecasts.append(res[0])\n",
    "        metrics.append(res[1])\n",
    "        print(i)\n",
    "    return pd.concat(forecasts), pd.DataFrame(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM 1\n",
      "0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'x_train_v2_0.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[208], line 58\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m#model = lgb.LGBMRegressor()\u001b[39;00m\n\u001b[0;32m     49\u001b[0m params2 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilstm\u001b[39m\u001b[38;5;124m'\u001b[39m: bilstm\n\u001b[0;32m     57\u001b[0m }\n\u001b[1;32m---> 58\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forecasts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#, True, np.array([1, 1, 1, 1, 1]))\u001b[39;00m\n\u001b[0;32m     60\u001b[0m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnew_results_v4_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     61\u001b[0m results[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnew_results_metrics_v4_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[166], line 213\u001b[0m, in \u001b[0;36mmodel_forecasts\u001b[1;34m(model, params_set, k, HPO, metric_weights)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 213\u001b[0m     data_x_train_val \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_train_v2_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    214\u001b[0m     num_lags \u001b[38;5;241m=\u001b[39m data_x_train_val\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, nn_wrapper_tf):\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    312\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    313\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    314\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    315\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[0;32m    316\u001b[0m     )\n\u001b[1;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1729\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1727\u001b[0m     is_text \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1728\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1729\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1730\u001b[0m     f,\n\u001b[0;32m   1731\u001b[0m     mode,\n\u001b[0;32m   1732\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1733\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1734\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1735\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1736\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1737\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1738\u001b[0m )\n\u001b[0;32m   1739\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1740\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\common.py:857\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    855\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    856\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    858\u001b[0m             handle,\n\u001b[0;32m    859\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    860\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    861\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    862\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    863\u001b[0m         )\n\u001b[0;32m    864\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    865\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    866\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'x_train_v2_0.csv'"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective':'regression',\n",
    "    'num_leaves': 100,\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'metric': 'l1',\n",
    "    'num_iterations': 200\n",
    "}\n",
    "\n",
    "params1 = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'num_leaves':  100,\n",
    "    'learning_rate': 0.1,\n",
    "    'metric':  ['l1', 'l2']\n",
    "}\n",
    "\n",
    "params3 = {\n",
    "    'num_epochs': 1,\n",
    "    'batch_size' : 30,\n",
    "    'hidden_size': 128,\n",
    "    'num_layers': [1, 2],\n",
    "    'drop': 0.25,\n",
    "    'learning_rate':  0.1,\n",
    "    'bilstm': True\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#tf.keras.utils.disable_interactive_logging()\n",
    "tf.keras.utils.enable_interactive_logging()\n",
    "seed(0)\n",
    "\n",
    "#model = lgb.LGBMRegressor()\n",
    "num_layers = 1\n",
    "k = 2\n",
    "model_types = ['LSTM', 'BiLSTM']\n",
    "for j in model_types:\n",
    "    bilstm = True if j == model_types[1] else False\n",
    "    \n",
    "    for i in range(num_layers, num_layers + 1):\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "        print(j, i)\n",
    "        model = nn_wrapper_tf(j)\n",
    "        #model = lgb.LGBMRegressor()\n",
    "        params2 = {\n",
    "        'num_epochs': 20,\n",
    "        'batch_size' : 64,\n",
    "        'hidden_size': 200,\n",
    "        'num_layers': i,\n",
    "        'drop': 0.3,\n",
    "        'learning_rate': 0.1,\n",
    "        'bilstm': bilstm\n",
    "        }\n",
    "        results = model_forecasts(model, params2, k)#, True, np.array([1, 1, 1, 1, 1]))\n",
    "\n",
    "        results[0].to_csv(f'new_results_v4_{j}_{i}_{k}.csv')\n",
    "        results[1].to_csv(f'new_results_metrics_v4_{j}_{i}_{k}.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.554</td>\n",
       "      <td>74.533706</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1999-03-19</td>\n",
       "      <td>1999-05-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.488</td>\n",
       "      <td>80.344355</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1999-06-01</td>\n",
       "      <td>1999-08-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         0         1      2          3     4           5  \\\n",
       "0           0  0.000145  0.000062  0.554  74.533706  0.48  1999-03-19   \n",
       "1           1  0.000103  0.000046  0.488  80.344355  0.60  1999-06-01   \n",
       "\n",
       "            6  \n",
       "0  1999-05-28  \n",
       "1  1999-08-10  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('new_results_metrics_v4_LSTM_1_2.csv')\n",
    "#print(df1[['2', '4']].apply(np.mean))\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.430</td>\n",
       "      <td>75.269500</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1999-03-19</td>\n",
       "      <td>1999-05-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.434</td>\n",
       "      <td>88.560639</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1999-06-01</td>\n",
       "      <td>1999-08-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.632</td>\n",
       "      <td>124.601431</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1999-08-11</td>\n",
       "      <td>1999-10-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.294</td>\n",
       "      <td>91.355377</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1999-10-21</td>\n",
       "      <td>1999-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.526</td>\n",
       "      <td>91.268426</td>\n",
       "      <td>0.66</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>2000-03-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         0         1      2           3     4           5  \\\n",
       "0           0  0.000145  0.000062  0.430   75.269500  0.60  1999-03-19   \n",
       "1           1  0.000100  0.000046  0.434   88.560639  0.54  1999-06-01   \n",
       "2           2  0.000155  0.000068  0.632  124.601431  0.54  1999-08-11   \n",
       "3           3  0.000073  0.000034  0.294   91.355377  0.50  1999-10-21   \n",
       "4           4  0.000218  0.000097  0.526   91.268426  0.66  2000-01-03   \n",
       "\n",
       "            6  \n",
       "0  1999-05-28  \n",
       "1  1999-08-10  \n",
       "2  1999-10-20  \n",
       "3  1999-12-31  \n",
       "4  2000-03-14  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['target'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfiltered_features.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiltered_features.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    312\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    313\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    314\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    315\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[0;32m    316\u001b[0m     )\n\u001b[1;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:5391\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5243\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m   5244\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m   5245\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5252\u001b[0m     errors: IgnoreRaise \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   5253\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5254\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   5255\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5256\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5389\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5390\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5391\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[0;32m   5392\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   5393\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   5394\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   5395\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   5396\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   5397\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[0;32m   5398\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   5399\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    312\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    313\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    314\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    315\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[0;32m    316\u001b[0m     )\n\u001b[1;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:4510\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4508\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   4509\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 4510\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4512\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[0;32m   4513\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:4551\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4549\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m   4550\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 4551\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4552\u001b[0m     indexer \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4554\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4555\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6972\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6970\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[0;32m   6971\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 6972\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(labels[mask])\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6973\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[0;32m   6974\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['target'] not found in axis\""
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('filtered_features.csv', index_col='Date').drop('target', axis=1)\n",
    "df.to_csv('filtered_features.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1509, 40)\n"
     ]
    }
   ],
   "source": [
    "num_lags = 40\n",
    "data_x_train_test = pd.read_csv(f'x_train_v2_{1}.csv').drop(['Unnamed: 0', 'Date'], axis = 1)\n",
    "print(np.array([data_x_train_test.values]).shape)\n",
    "tick = ['IXIC_Close', 'DX-Y.NYB_Close', 'VIX_Close']\n",
    "addit_data = []\n",
    "\n",
    "for t in tick:\n",
    "    add_data = pd.read_csv(f'x_train_v2_{t}_{1}.csv', index_col='Date')\n",
    "    add_data['Date'] = add_data.index\n",
    "    add_data.index = np.array(range(add_data.shape[0]))\n",
    "    add_data = add_data.drop(['Unnamed: 0', 'Date'], axis = 1)\n",
    "    addit_data.append(add_data.values)\n",
    "data_x_train_test = np.concatenate([np.array([data_x_train_test.values]), np.array(addit_data)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1509, 40)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x_train_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1509, 40)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(addit_data).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
